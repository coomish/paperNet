{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Notebook to experiment with topic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/ellen/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to /Users/ellen/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### Local functions\n",
    "import topic_model\n",
    "\n",
    "### Libraries\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "## Spacy\n",
    "import spacy\n",
    "spacy.load('en')\n",
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "\n",
    "## nltk\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 Load PDFs from folder 'papers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hybrid Recommender Systems.pdf',\n 'Information Theory - Tutorial.pdf',\n 'Geometric Understanding of Deep Learning.pdf',\n '1802.05968v2.pdf',\n 'Time Series Feature Extraction.pdf',\n 'An Introduction to DRL.pdf',\n 'Multitask Learning as Multiobjective Optimization.pdf',\n 'GANs.pdf',\n 'Introduction to Transfer Learning.pdf',\n 'Deep CNN Design Patterns.pdf']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_file_names(folder='papers'):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param folder: string, name of folder where the papers are contained\n",
    "    :return: string foldername, list filenames\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    file_names = os.listdir(folder)\n",
    "    return folder, file_names\n",
    "\n",
    "\n",
    "foldername, filenames = getFileNames('papers')\n",
    "\n",
    "filenames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Retrieve the text from the PDF files\n",
    "\n",
    "We use pdf miner to retrieve the text from the filenames in our 'papers' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import XMLConverter, TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "import io\n",
    "\n",
    "# TODO check if this works for multiple papers too\n",
    "# TODO e.g. is it possible to create a list of tokens for each? \n",
    "# TODO what is the best way to do this?\n",
    "# TODO is this already sth that should be returned in the end?\n",
    "\n",
    "\n",
    "def pdfparser(filename):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param data: filename (string)\n",
    "    :return: text data (string), the complete text in the document(s)\n",
    "    \"\"\"\n",
    "    fp = open(filename, 'rb')\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    # Create a PDF interpreter object.\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    # Process each page contained in the document.\n",
    "\n",
    "    for page in PDFPage.get_pages(fp):\n",
    "        interpreter.process_page(page)\n",
    "        data = retstr.getvalue()\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pdfparser(foldername+'/'+filenames[0])  # example, just for the first paper\n",
    "\n",
    "# TODO cleansing operations directly on raw text\n",
    "\n",
    "text= text.replace(\"\\n\",\" \")\n",
    "text = re.sub(r'\\d+', '', text)\n",
    "text = text.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
    "text = text.strip()\n",
    "text = re.sub(r'\\b\\w{1,3}\\b', '', text)\n",
    "# text = [tok for tok in text if tok not in STOPLIST and tok not in punctuations]\n",
    "# text = ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hybrid Recommender Systems  Systematic Literature Review  Department  Control  Computer Engineering Politecnico  Torino Corso Duca degli Abruzzi    Torino  Erion ¸  Maurizio Morisio  Abstract Recommender systems  software tools used  generate  provide suggestions  items  other entities   users  exploiting various strategies Hybrid recommender systems combine   more recommendation strategies  different ways  beneﬁt from their  plementary advantages This systematic literature review presents  state     hybrid recommender systems   last decade    ﬁrst quantitative review work completely  cused  h'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0:600]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 Quick data exploration: check of top x words with nltk freqdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = topic_model.prepare_text_for_lda(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hybrid;191\nsystem;181\nrecommendation;167\nstudy;152\nrecommender;123\nauthor;78\ntechnique;75\nproblem;68\nitem;61\nconference;59\ndifferent;58\ndomain;58\nbase;57\naccuracy;53\ninternational;53\nuser;52\nusing;47\nresult;47\nevaluation;46\ninformation;46\n"
     ]
    }
   ],
   "source": [
    "# Calculate frequency distribution\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "\n",
    "# Output top 20 words\n",
    "\n",
    "for word, frequency in fdist.most_common(20):\n",
    "    print(u'{};{}'.format(word, frequency))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 Preprocessing: remove punctuation, newline chars, stopwords etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper contains 96995 tokens in total\nHere are the tokens at position 400-420: \n\n 'rent ways  beneﬁt fr ' \n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After conversion to <class 'spacy.tokens.doc.Doc'> the paper contains 15848 tokens\nHere are the tokens at position 400-420:  \n\n'problem Recommender Systems    such tools that emerged     They  commonly deﬁned  software tools  techniques used  '\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en')\n",
    "\n",
    "# text is a string of all the separate tokens\n",
    "print(\"The paper contains \" + str(len(text)) + \" tokens in total\")\n",
    "print(\"Here are the tokens at position 400-420: \\n\\n '\" + str(text[400:420]) + \" ' \\n\")\n",
    "\n",
    "# create spacy tokens\n",
    "tokens = nlp(text)\n",
    "print(\"After conversion to \" + str(type(tokens)) + \" the paper contains \" + str(len(tokens)) + \" tokens\")\n",
    "print(\"Here are the tokens at position 400-420:  \\n\\n'\" + str(tokens[400:420]) + \"'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4a) Remove stopwords and punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'w present state hybrid recommender system decade ﬁrst quantitative review work completely cuse hybrid recommender address relevant problem consider present associate datum mining recommendation technique overcome explore hybridization class hybrid recommender belong application domain evaluation process propose future research direction base ﬁnding study combine collaborative ﬁltering technique we'"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "STOPLIST = set(stopwords.words('english'))\n",
    "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-\", \"...\", \"”\", \"”\"]\n",
    "punctuations = string.punctuation\n",
    "\n",
    "\n",
    "def clean_up(tokens):\n",
    "    clean_tokens = []\n",
    "    for each in tokens:\n",
    "        if each.is_stop == False:\n",
    "            if each.is_punct == False:\n",
    "                clean_tokens.append(each)\n",
    "    final_tokens = [tok.lemma_.lower().strip() for tok in clean_tokens if tok.lemma_ != '-PRON-']\n",
    "    final_tokens = [tok for tok in final_tokens if tok not in STOPLIST and tok not in punctuations]\n",
    "    final_tokens = ' '.join(final_tokens)\n",
    "    return final_tokens\n",
    "\n",
    "def further_clean_up(tokens):\n",
    "    tokens = re.sub(r'\\d+', '', tokens)\n",
    "    tokens = tokens.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
    "    tokens = tokens.strip()\n",
    "    tokens = re.sub(r'\\b\\w{1,3}\\b', '', tokens)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# apply the clean_up functions to the spacy nlp tokens\n",
    "clean_tokens = clean_up(tokens)\n",
    "clean_tokens = further_clean_up(clean_tokens)\n",
    "clean_tokens = clean_tokens.strip()\n",
    "clean_tokens[400:800]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collect demo graphic information need online privacy concern limit utilization combine recommender reinforce technique well quality knowledgebased filtering  knowledge user item reason item meet user requirement generate recommendation accordingly special type kbfs constraintbase capable recommend complex item rarely   house manifest important constrain user price possible successfully domain item usersystem interaction datum available people rarely house early recommender system tapestry manual mail system ﬁrst computerize prototype apply collaborative ﬁltering approach emerge grouplens recommendation engine ﬁnde news article author present detailed analysis evaluation bellcore video recommender algorithm implementation embed mosaic browser interface ringo taste similarity provide personalized music recommendation prototype like newsweeder infofinder recommend news document base item attribute late important commercial prototype come amazoncom recommender popular researcher start combine recommendation strategy different  build hybrid consider review hybrid strategy goal reinforce advantage reduce disadvantage limitation ﬁrst metalevel recommender section suggest website incorporated combination user similar website preference website similar content work follow shortly hybrid establish recommendation approach continuously grow industrial interest recent promising domain mobile social follow similar increase academic interest recsys annual conference signiﬁcant event present discuss research work burke ﬁrst qualitative survey address hybrid thor analyze advantage disadvantage different recommendation strategy vide comprehensive taxonomy classifying  combine"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unfortunately this returns a string again so we need to apply nlp again \n",
    "\n",
    "final_tokens = nlp(clean_tokens)\n",
    "final_tokens[400:600]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b) POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic ADJ\nassumption NOUN\npeople NOUN\nsimilar ADJ\ntaste NOUN\npast ADP\nsimilar ADJ\ntaste NOUN\nfuture ADJ\nearly ADJ\ndeﬁnition NOUN\ncollaboration NOUN\npeople NOUN\nhelp VERB\nperform VERB\nﬁltere NOUN\nrecord NOUN\nreaction NOUN\ndocument NOUN\nread VERB\napproach NOUN\n  SPACE\nrating NOUN\nform NOUN\nuser NOUN\ngenerate VERB\nfeedback NOUN\nspot NOUN\ntaste NOUN\ncommonality NOUN\n"
     ]
    }
   ],
   "source": [
    "# for token in doc:\n",
    "#     print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "#             token.shape_, token.is_alpha, token.is_stop)\n",
    "\n",
    "for token in final_tokens:\n",
    "    # print(token.pos_)\n",
    "    pass\n",
    "\n",
    "for word in final_tokens[300:330]:  \n",
    "    print(word.text,  word.pos_)\n",
    "\n",
    "for noun in final_tokens[300:330].noun_chunks:  \n",
    "    # print(noun.text)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4c) Lemmatization\n",
    "\n",
    "Now that we are finished with the data cleaning, we can apply lemmatization to the final tokens\n",
    "\n",
    "Lemmatization is sufficient for our use case, we do not apply stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['form', 'hybrid', 'present', 'hybrid', 'prototype', 'fall', 'hybridization', 'class', 'taxonomy', 'early', 'exploratory', 'work', 'experiment', 'combine', 'personalize', 'agent', 'opinion', 'community', 'member', 'framework', 'conduct', 'conclude', 'combination', 'produce', 'highquality', 'recommendation', 'good', 'result', 'achieve', 'large', 'datum', 'user', 'community', 'review', 'work', 'generic', 'address', 'general', 'focus', 'type', 'reﬂect', 'increase', ' ', 'ﬁeld', 'quantitative', 'term', 'author', 'perform', 'review', 'work', 'journal', 'conference', 'publication', 'peak', 'publication', 'period', 'work', 'consider', 'onethird', 'analyze', 'period', 'emphasize', 'fact', 'current', 'hybrid', 'incorporate', 'location', 'information', 'exist', 'recom', 'mendation', 'algorithm', 'highlight', 'proper', 'combination', 'exist', 'method', 'different', 'form', 'datum', 'evaluate', 'characteristic', 'diversity', 'novelty', 'accuracy', 'future', 'trend', 'author', 'review', 'recommender', 'system', 'article', 'publish', 'journal', 'similarly', 'report', 'rapid', 'increase', 'publication', 'predict', 'increase', 'interest', ' ', 'exist', 'recommendation', 'method', 'social', 'network', 'analysis', 'provide', 'recommendation', 'review', 'paper', 'summarize', 'state', 'hybrid', 'year', 'follow', 'systematic', 'methodology', 'analyze', 'interpret', 'available', 'fact', 'relate', 'research', 'question', 'deﬁne', 'methodology', 'deﬁne', 'provide', 'unbiased', 'reproducible', 'undertaking', 'review', 'work', 'unlike', 'review', 'work', 'focus', 'type', 'systematic', 'literature', 'review', 'ﬁrst', 'quantitative', 'work', 'entirely', 'focus', 'recent', 'hybrid', 'publication', 'reason', 'possible', 'direct', 'basis', 'compare', 'result', 'provide', 'comparison', 'result', 'certain', 'aspect', 'hybrid', 'differ', 'type', 'general', 'idea', 'percentage', 'total', 'publication', 'address', 'hybrid', 'examine', 'survey', 'work', 'general', 'author', 'review', 'work', 'paper', 'publish', 'computer', 'science', 'information', 'system', 'conference', 'proceed', 'journal', 'result', 'hybrid', 'recommendation', 'paradigm', 'study', 'object', 'review', 'literature', 'consider', 'relevant', 'problem']\n"
     ]
    }
   ],
   "source": [
    "# TODO remove empty values\n",
    "\n",
    "def lemmatizer(tokens):\n",
    "    spacy_lemmas = []\n",
    "    for word in tokens:\n",
    "        spacy_lemmas.append(word.lemma_)\n",
    "    return spacy_lemmas\n",
    " \n",
    "   \n",
    "lemmatized_tokens = lemmatizer(final_tokens)\n",
    "\n",
    "print(lemmatized_tokens[600:800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "# \n",
    "# stemmer = SnowballStemmer(language='english')\n",
    "# \n",
    "# # test_tokens = ['compute', 'computer', 'computed', 'computing']\n",
    "# # document = sp(spacy_tokens[200:300])\n",
    "# id_sequence = map(lam'bda x: x.orth, [token for token in spacy_tokens[200:300]])\n",
    "# text = map(lambda x: sp.vocab[x].text, [id for id in id_sequence])\n",
    "# \n",
    "# for token in text:  \n",
    "#     print(token + ' --> ' + stemmer.stem(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Analyze ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def word_grams(words, number):\n",
    "    s = []\n",
    "    for ngram in ngrams(words, number):\n",
    "            s.append(' '.join(str(i) for i in ngram))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('recommender system', 89),\n ('hybrid recommender', 34),\n ('international conference', 30),\n ('ieee ieee', 23),\n ('ieee international', 20),\n ('datum sparsity', 19),\n ('hybrid recommendation', 19),\n ('datum mining', 18),\n ('future work', 18),\n ('find study', 18),\n ('recommendation strategy', 17),\n ('application domain', 17),\n ('recommendation technique', 16),\n ('user proﬁle', 16),\n ('research question', 16),\n ('hybrid approach', 15),\n ('hybridization class', 14),\n ('science direct', 14),\n ('association rule', 14),\n ('international journal', 14)]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = word_grams(lemmatized_tokens, 2)\n",
    "from collections import Counter\n",
    "count_grams = Counter(bigrams)\n",
    "count_grams.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Creating a model to compare similarities of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
