{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Notebook to experiment with topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/ellen/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to /Users/ellen/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.load('en')\n",
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/ellen/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to /Users/ellen/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import load_pdfs, read_pdf, topic_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Load PDFs from folder 'papers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileNames():\n",
    "    foldername = 'papers'\n",
    "    file_names = os.listdir(foldername)\n",
    "    return foldername, file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Recommender Systems.pdf\n\n\nInformation Theory - Tutorial.pdf\n\n\nGeometric Understanding of Deep Learning.pdf\n\n\n1802.05968v2.pdf\n\n\nTime Series Feature Extraction.pdf\n\n\nAn Introduction to DRL.pdf\n\n\nMultitask Learning as Multiobjective Optimization.pdf\n\n\nGANs.pdf\n\n\nIntroduction to Transfer Learning.pdf\n\n\nDeep CNN Design Patterns.pdf\n\n\n"
     ]
    }
   ],
   "source": [
    "foldername, filenames = load_pdfs.getFileNames()\n",
    "\n",
    "try:\n",
    "    for each in filenames:\n",
    "        print(each)\n",
    "        print('\\n')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Retrieve the text from the PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    resource_manager = PDFResourceManager()\n",
    "    fake_file_handle = io.BytesIO()\n",
    "    converter = TextConverter(resource_manager, fake_file_handle)\n",
    "    page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "\n",
    "    with open(pdf_path, 'rb') as fh:\n",
    "        for page in PDFPage.get_pages(fh,\n",
    "                                      caching=True,\n",
    "                                      check_extractable=True):\n",
    "            page_interpreter.process_page(page)\n",
    "\n",
    "        text = fake_file_handle.getvalue()\n",
    "\n",
    "    # close open handles\n",
    "    converter.close()\n",
    "    fake_file_handle.close()\n",
    "\n",
    "    if text:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_pdf.extract_text_from_pdf(foldername+'/'+filenames[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'InformationTheory:ATutorialIntroductionJamesVStone,PsychologyDepartment,UniversityofShe\\xef\\xac\\x83eld,England.j.v.stone@she\\xef\\xac\\x83eld.ac.ukFile:mainInformationTheoryJVStonev3.texAbstractShannon\\xe2\\x80\\x99smathematicaltheoryofcommunicationde\\xef\\xac\\x81nesfundamentallimitsonhowmuchinformationcanbetransmittedbetweenthedi\\xef\\xac\\x80erentcomponentsofanyman-madeorbiologicalsystem.ThispaperisaninformalbutrigorousintroductiontothemainideasimplicitinShannon\\xe2\\x80\\x99stheory.Anannotatedreadinglistisprovidedforfurtherreading.1IntroductionIn1948,ClaudeShannonpublishedapapercalledAMathematicalTheoryofCommunication[1].Thispaperheraldedatransformationinourunderstandingofinformation.BeforeShannon\\xe2\\x80\\x99spaper,informationhadbeenviewedasakindofpoorlyde\\xef\\xac\\x81nedmiasmic\\xef\\xac\\x82uid.ButafterShannon\\xe2\\x80\\x99spaper,itbecameapparentthatinformationisawell-de\\xef\\xac\\x81nedand,aboveall,measurablequantity.Indeed,asnotedbyShannon,Abasicideaininformationtheoryisthatinformationcanbetreatedverymuchlikeaphysicalquantity,suchasmassorenergy.CaudeShannon,1985.EncodingMessagesChannelNoiseDecoding\\xce\\xb7xyMessagesFigure1:Thecommunicationchannel.Amessage(data)isencodedbeforebeingusedasinputtoacommunicationchannel,whichaddsnoise.Thechanneloutputisdecodedbyareceivertorecoverthemessage.1arXiv:1802.05968v2  [cs.IT]  20 Feb 2018\\x0cInformationtheoryde\\xef\\xac\\x81nesde\\xef\\xac\\x81nite,unbreachablelimitsonpreciselyhowmuchinformationcanbecommunicatedbetweenanytwocomponentsofanysystem,whetherthissystemisman-madeornatural.Thetheoremsofinformationtheoryaresoimportantthattheydeservetoberegardedasthelawsofinformation[2,3,4].Thebasiclawsofinformationcanbesummarisedasfollows.Foranycommunicationchannel(Figure1):1)thereisade\\xef\\xac\\x81niteupperlimit,thechannelcapacity,totheamountofinformationthatcanbecommunicatedthroughthatchannel,2)thislimitshrinksastheamountofnoiseinthechannelincreases,3)thislimitcanverynearlybereachedbyjudiciouspackaging,orencoding,ofdata.2FindingaRoute,BitbyBitInformationisusuallymeasuredinbits,andonebitofinformationallowsyoutochoosebetweentwoequallyprobable,orequiprobable,alternatives.Inordertounderstandwhythisisso,imagineyouarestandingattheforkintheroadatpointAinFigure2,andthatyouwanttogettothepointmarkedD.TheforkatArepresentstwoequiprobablealternatives,soifItellyoutogoleftthenyouhavereceivedonebitofinformation.Ifwerepresentmyinstructionwithabinarydigit(0=leftand1=right)thenthisbinarydigitprovidesyouwithonebitofinformation,whichtellsyouwhichroadtochoose.Nowimaginethatyoucometoanotherfork,atpointBinFigure2.Again,abinarydigit(1=right)providesonebitofinformation,allowingyoutochoosethecorrectroad,whichleadstoC.NotethatCisoneoffourpossibleinterimdestinationsthatyoucouldABD0 1 1 = 30 0 0 = 00 0 1 = 10 1 0 = 21 0 0 = 41 0 1 = 51 1 0 = 61 1 1 = 7C---------------    ----------------------LeftRight01001111110000Figure2:Foratravellerwhodoesnotknowtheway,eachforkintheroadrequiresonebitofinformationtomakeacorrectdecision.The0sand1sontheright-handsidesummarisetheinstructionsneededtoarriveateachdestination;aleftturnisindicatedbya0andarightturnbya1.2\\x0chavereachedaftermakingtwodecisions.Thetwobinarydigitsthatallowyoutomakethecorrectdecisionsprovidedtwobitsofinformation,allowingyoutochoosefromfour(equiprobable)alternatives;4equals2\\xc3\\x972=22.Athirdbinarydigit(1=right)providesyouwithonemorebitofinformation,whichallowsyoutoagainchoosethecorrectroad,leadingtothepointmarkedD.TherearenoweightroadsyoucouldhavechosenfromwhenyoustartedatA,sothreebinarydigits(whichprovideyouwiththreebitsofinformation)allowyoutochoosefromeightequiprobablealternatives,whichalsoequals2\\xc3\\x972\\xc3\\x972=23=8.Wecanrestatethisinmoregeneraltermsifweusentorepresentthenumberofforks,andmtorepresentthenumberof\\xef\\xac\\x81naldestinations.Ifyouhavecometonforksthenyouhavee\\xef\\xac\\x80ectivelychosenfromm=2n\\xef\\xac\\x81naldestinations.Becausethedecisionateachforkrequiresonebitofinformation,nforksrequirenbitsofinformation.Viewedfromanotherperspective,iftherearem=8possibledestinationsthenthenumberofforksisn=3,whichisthelogarithmof8.Thus,3=log28isthenumberofforksimpliedbyeightdestinations.Moregenerally,thelogarithmofmisthepowertowhich2mustberaisedinordertoobtainm;thatis,m=2n.Equivalently,givenanumberm,whichwewishtoexpressasalogarithm,n=log2m.Thesubscript2indicatesthatweareusinglogstothebase2(alllogarithmsinthisbookusebase2unlessstatedotherwise).3BitsAreNotBinaryDigitsThewordbitisderivedfrombinarydigit,butabitandabinarydigitarefundamentallydi\\xef\\xac\\x80erenttypesofquantities.Abinarydigitisthevalueofabinaryvariable,whereasabitisanamountofinformation.Tomistakeabinarydigitforabitisacategoryerror.Inthiscase,thecategoryerrorisnotasbadasmistakingmarzipanforjustice,butitisanalogoustomistakingapint-sizedbottleforapintofmilk.Justasabottlecancontainbetweenzeroandonepint,soabinarydigit(whenaveragedoverbothofitspossiblestates)canconveybetweenzeroandonebitofinformation.4InformationandEntropyConsideracoinwhichlandsheadsup90%ofthetime(i.e.p(xh)=0.9).Whenthiscoinis\\xef\\xac\\x82ipped,weexpectittolandheadsup(x=xh),sowhenitdoessowearelesssurprisedthanwhenitlandstailsup(x=xt).Themoreimprobableaparticularoutcomeis,themoresurprisedwearetoobserveit.Ifweuselogarithmstothebase2thentheShannon3\\x0cinformationorsurprisalofeachoutcome,suchasaheadxh,ismeasuredinbits(seeFigure3a)Shannoninformation=log1p(xh)bits,(1)whichisoftenexpressedas:information=\\xe2\\x88\\x92logp(xh)bits.EntropyisAverageShannonInformation.Wecanrepresenttheoutcomeofacoin\\xef\\xac\\x82ipastherandomvariablex,suchthataheadisx=xhandatailisx=xt.Inpractice,wearenotusuallyinterestedinthesurpriseofaparticularvalueofarandomvariable,butweareinterestedinhowmuchsurprise,onaverage,isassociatedwiththeentiresetofpossiblevalues.Theaveragesurpriseofavariablexisde\\xef\\xac\\x81nedbyitsprobabilitydistributionp(x),andiscalledtheentropyofp(x),representedasH(x).TheEntropyofaFairCoin.Theaverageamountofsurpriseaboutthepossibleoutcomesofacoin\\xef\\xac\\x82ipcanbefoundasfollows.Ifacoinisfairorunbiasedthenp(xh)=p(xt)=0.5thentheShannoninformationgainedwhenaheadoratailisobservedislog1/0.5=1bit,sotheaverageShannoninformationgainedaftereachcoin\\xef\\xac\\x82ipisalso1bit.Becauseentropyisde\\xef\\xac\\x81nedasaverageShannoninformation,theentropyofafaircoinisH(x)=1bit.TheEntropyofanUnfair(Biased)Coin.Ifacoinisbiasedsuchthattheprobabilityofaheadisp(xh)=0.9thenitiseasytopredicttheresultofeachcoin\\xef\\xac\\x82ip(i.e.with90%accuracyifwepredictaheadforeach\\xef\\xac\\x82ip).IftheoutcomeisaheadthentheamountofShannoninformationgainedislog(1/0.9)=0.15bits.Butiftheoutcomeisatailthen00.20.40.60.81 p(x)01234567Surprise = log 1/p(x)(a)(b)Figure3:a)Shannoninformationassurprise.Valuesofxthatarelessprobablehavelargervaluesofsurprise,de\\xef\\xac\\x81nedaslog2(1/p(x))bits.b)GraphofentropyH(x)versuscoinbias(probabilityp(xh)ofahead).TheentropyofacoinistheaverageamountofsurpriseorShannoninformationinthedistributionofpossibleoutcomes(i.e.headsandtails).4\\x0ctheamountofShannoninformationgainedislog(1/0.1)=3.32bits.Noticethatmoreinformationisassociatedwiththemoresurprisingoutcome.Giventhattheproportionof\\xef\\xac\\x82ipsthatyieldaheadisp(xh),andthattheproportionof\\xef\\xac\\x82ipsthatyieldatailisp(xt)(wherep(xh)+p(xt)=1),theaveragesurpriseisH(x)=p(xh)log1p(xh)+p(xt)log1p(xt),(2)whichcomestoH(x)=0.469bits,asinFigure3b.Ifwede\\xef\\xac\\x81neatailasx1=xtandaheadasx2=xhthenEquation2canbewrittenasH(x)=2(cid:88)i=1p(xi)log1p(xi)bits.(3)Moregenerally,arandomvariablexwithaprobabilitydistributionp(x)={p(x1),...,p(xm)}hasanentropyofH(x)=m(cid:88)i=1p(xi)log1p(xi)bits.(4)Thereasonthisde\\xef\\xac\\x81nitionmattersisbecauseShannon\\xe2\\x80\\x99ssourcecodingtheorem(seeSection7)guaranteesthateachvalueofthevariablexcanberepresentedwithanaverageof(justover)H(x)binarydigits.However,ifthevaluesofconsecutivevaluesofarandomvariablearenotindependenttheneachvalueismorepredictable,andthereforelesssurprising,whichreducestheinformation-carryingcapability(i.e.entropy)ofthevariable.Thisiswhyitisimportanttospecifywhetherornotconsecutivevariablevaluesareindependent.InterpretingEntropy.IfH(x)=1bitthenthevariablexcouldbeusedtorepresentm=2H(x)or2equiprobablevalues.Similarly,ifH(x)=0.469bitsthenthevariablex(a)2345678910111200.020.040.060.080.10.120.140.160.18Outcome valueOutcome probability(b)Figure4:(a)Apairofdice.(b)Histogramofdiceoutcomevalues.5\\x0ccouldbeusedtorepresentm=20.469or1.38equiprobablevalues;asifwehadadiewith1.38sides.At\\xef\\xac\\x81rstsight,thisseemslikeanoddstatement.Nevertheless,translatingentropyintoanequivalentnumberofequiprobablevaluesservesasanintuitiveguidefortheamountofinformationrepresentedbyavariable.DicingWithEntropy.Throwingapairof6-sideddiceyieldsanoutcomeintheformofanorderedpairofnumbers,andthereareatotalof36equiprobableoutcomes,asshowninTable1.Ifwede\\xef\\xac\\x81neanoutcomevalueasthesumofthispairofnumbersthentherearem=11possibleoutcomevaluesAx={2,3,4,5,6,7,8,9,10,11,12},representedbythesymbolsx1,...,x11.TheseoutcomevaluesoccurwiththefrequenciesshowninFigure4bandTable1.Dividingthefrequencyofeachoutcomevalueby36yieldstheprobabilityPofeachoutcomevalue.UsingEquation4,wecanusethese11probabilitiesto\\xef\\xac\\x81ndtheentropyH(x)=p(x1)log1p(x1)+p(x2)log1p(x2)+\\xc2\\xb7\\xc2\\xb7\\xc2\\xb7+p(x11)log1p(x11)=3.27bits.Usingtheinterpretationdescribedabove,avariablewithanentropyof3.27bitscanrepresent23.27=9.65equiprobablevalues.EntropyandUncertainty.Entropyisameasureofuncertainty.Whenouruncertaintyisreduced,wegaininformation,soinformationandentropyaretwosidesofthesamecoin.However,informationhasarathersubtleinterpretation,whichcaneasilyleadtoconfusion.Averageinformationsharesthesamede\\xef\\xac\\x81nitionasentropy,butwhetherwecallagivenquantityinformationorentropyusuallydependsonwhetheritisbeinggiventousortakenSymbolSumOutcomeFrequencyPSurprisalx121:110.035.17x231:2,2:120.064.17x341:3,3:1,2:230.083.59x452:3,3:2,1:4,4:140.113.17x562:4,4:2,1:5,5:1,3:350.142.85x673:4,4:3,2:5,5:2,1:6,6:160.172.59x783:5,5:3,2:6,6:2,4:450.142.85x893:6,6:3,4:5,5:440.113.17x9104:6,6:4,5:530.083.59x10115:6,6:520.064.17x11126:610.035.17Table1:Apairofdicehave36possibleoutcomes.Sum:outcomevalue,totalnumberofdotsforagiventhrowofthedice.Outcome:orderedpairofdicenumbersthatcouldgenerateeachsymbol.Freq:numberofdi\\xef\\xac\\x80erentoutcomesthatcouldgenerateeachoutcomevalue.P:theprobabilitythatthepairofdiceyieldagivenoutcomevalue(freq/36).Surprisal:Plog(1/P)bits.6\\x0caway.Forexample,ifavariablehashighentropythenourinitialuncertaintyaboutthevalueofthatvariableislargeandis,byde\\xef\\xac\\x81nition,exactlyequaltoitsentropy.Ifwearetoldthevalueofthatvariablethen,onaverage,wehavebeengivenanamountofinformationequaltotheuncertainty(entropy)weinitiallyhadaboutitsvalue.Thus,receivinganamountofinformationisequivalenttohavingexactlythesameamountofentropy(uncertainty)takenaway.5EntropyofContinuousVariablesFordiscretevariables,entropyiswell-de\\xef\\xac\\x81ned.However,forallcontinuousvariables,entropyise\\xef\\xac\\x80ectivelyin\\xef\\xac\\x81nite.Considerthedi\\xef\\xac\\x80erencebetweenadiscretevariablexdwithnpossiblevaluesandacontinuousvariablexcwithanuncountablyin\\xef\\xac\\x81nitenumberofpossiblevalues;forsimplicity,assumethatallvaluesareequallyprobable.TheprobabilityofobservingeachvalueofthediscretevariableisPd=1/m,sotheentropyofxdisH(xd)=logm.Incontrast,theprobabilityofobservingeachvalueofthecontinuousvariableisPc=1/\\xe2\\x88\\x9e=0,sotheentropyofxcisH(xc)=log\\xe2\\x88\\x9e=\\xe2\\x88\\x9e.Inonerespect,thismakessense,becauseeachvalueofofacontinuousvariableisimplicitlyspeci\\xef\\xac\\x81edwithin\\xef\\xac\\x81niteprecision,fromwhichitfollowsthattheamountofinformationconveyedbyeachvalueisin\\xef\\xac\\x81nite.However,thisresultimpliesthatallcontinuousvariableshavethesameentropy.Inordertoassigndi\\xef\\xac\\x80erentvaluestodi\\xef\\xac\\x80erentvariables,allin\\xef\\xac\\x81nitetermsaresimplyignored,whichyieldsthedi\\xef\\xac\\x80erentialentropyH(xc)=(cid:90)p(xc)log1p(xc)dxc.(5)Itisworthnotingthatthetechnicaldi\\xef\\xac\\x83cultiesassociatedwithentropyofcontinuousvariablesdisappearforquantitieslikemutualinformation,whichinvolvethedi\\xef\\xac\\x80erencebetweentwoentropies.Forconvenience,weusethetermentropyforbothcontinuousanddiscretevariablesbelow.6MaximumEntropyDistributionsAdistributionofvaluesthathasasmuchentropy(information)astheoreticallypossibleisamaximumentropydistribution.Maximumentropydistributionsareimportantbecause,ifwewishtouseavariabletotransmitasmuchinformationaspossiblethenwehadbettermakesureithasmaximumentropy.Foragivenvariable,thepreciseformofitsmaximumentropydistributiondependsontheconstraintsplacedonthevaluesofthatvariable[3].Itwillproveusefultosummarisethreeimportantmaximumentropydistributions.Thesearelistedinorderofdecreasingnumbersofconstraintsbelow.7\\x0cTheGaussianDistribution.Ifavariablexhasa\\xef\\xac\\x81xedvariance,butisotherwiseunconstrained,thenthemaximumentropydistributionistheGaussiandistribution(Figure5a).Thisisparticularlyimportantintermsofenergye\\xef\\xac\\x83ciencybecausenootherdistributioncanprovideasmuchinformationatalowerenergycostperbit.IfavariablehasaGaussianornormaldistributionthentheprobabilityofobservingaparticularvaluexisp(x)=1\\xe2\\x88\\x9a2\\xcf\\x80vxe\\xe2\\x88\\x92(\\xc2\\xb5x\\xe2\\x88\\x92x)2/(2vx),(6)wheree=2.7183.Thisequationde\\xef\\xac\\x81nesthebell-shapedcurveinFigure5a.Theterm\\xc2\\xb5xisthemeanofthevariablex,andde\\xef\\xac\\x81nesthecentralvalueofthedistribution;weassumethatallvariableshaveameanofzero(unlessstatedotherwise).Thetermvxisthevarianceofthevariablex,whichisthesquareofthestandarddeviation\\xcf\\x83xofx,andde\\xef\\xac\\x81nesthewidthofthebellcurve.Equation6isaprobabilitydensityfunction,and(strictlyspeaking)p(x)istheprobabilitydensityofx.TheExponentialDistribution.Ifavariablehasnovaluesbelowzero,andhasa\\xef\\xac\\x81xedmean\\xc2\\xb5,butisotherwiseunconstrained,thenthemaximumentropydistributionisexponential,p(x)=1\\xc2\\xb5e\\xe2\\x88\\x92x/\\xc2\\xb5,(7)whichhasavarianceofvar(x)=\\xc2\\xb52,asshowninFigure5b.TheUniformDistribution.Ifavariablehasa\\xef\\xac\\x81xedlowerboundxminandupperboundxmax,butisotherwiseunconstrained,thenthemaximumentropydistributionisuniform,p(x)=1/(xmax\\xe2\\x88\\x92xmin),(8)-3-2-1012300.050.10.150.20.250.30.350.4p(x)x\\xcf\\x83(a)01234500.10.20.30.40.50.60.70.80.91p(x)x(b)-0.500.511.522.500.10.20.30.40.5p(x)x(c)Figure5:Maximumentropydistributions.a)Gaussiandistribution,withmean\\xc2\\xb5=0andastandarddeviation\\xcf\\x83=1(Equation6).b)Exponentialdistribution,withmeanindicatedbytheverticalline(Equation7).c)Auniformdistributionwitharangebetweenzeroandtwo(Equation8).8\\x0cwhichhasavariance(xmax\\xe2\\x88\\x92xmin)2/12,asshowninFigure5c.7ChannelCapacityAveryimportant(andconvenient)channelistheadditivechannel.Asencodedvaluespassthroughanadditivechannel,noise\\xce\\xb7(eta)isadded,sothatthechanneloutputisanoisyversionyofthechannelinputxy=x+\\xce\\xb7.(9)ThechannelcapacityCisthemaximumamountofinformationthatachannelcanprovideatitsoutputabouttheinput.Therateatwhichinformationistransmittedthroughthechanneldependsontheentropiesofthreevariables:1)theentropyH(x)oftheinput,2)theentropyH(y)oftheoutput,3)theentropyH(\\xce\\xb7)ofthenoiseinthechannel.Iftheoutputentropyishighthenthisprovidesalargepotentialforinformationtransmission,andtheextenttowhichthispotentialisrealiseddependsontheinputentropyandthelevelofnoise.Ifthenoiseislowthentheoutputentropycanbeclosetothechannelcapacity.However,channelcapacitygetsprogressivelysmallerasthenoiseincreases.Capacityisusuallyexpressedinbitsperusage(i.e.bitsperoutput),orbitspersecond(bits/s).Channel  Capacity Channel  Capacity Binary digits transmitted Noise Noisy Channel Noiseless Channel Figure6:Thechannelcapacityofnoiselessandnoisychannelsisthemaximumrateatwhichinformationcanbecommunicated.Ifanoiselesschannelcommunicatesdataat10binarydigits/sthenitscapacityisC=10bits/s.Thecapacityofanoiselesschannelisnumericallyequaltotherateatwhichitcommunicatesbinarydigits,whereasthecapacityofanoisychannelislessthanthisbecauseitislimitedbytheamountofnoiseinthechannel.9\\x0c8Shannon\\xe2\\x80\\x99sSourceCodingTheoremShannon\\xe2\\x80\\x99ssourcecodingtheorem,describedbelow,appliesonlytonoiselesschannels.Thistheoremisreallyaboutre-packaging(encoding)databeforeitistransmitted,sothat,whenitistransmitted,everydatumconveysasmuchinformationaspossible.Thistheoremishighlyrelevanttothebiologicalinformationprocessingbecauseitde\\xef\\xac\\x81nesde\\xef\\xac\\x81nitelimitstohowe\\xef\\xac\\x83cientlysensorydatacanbere-packaged.Weconsiderthesourcecodingtheoremusingbinarydigitsbelow,butthelogicoftheargumentappliesequallywelltoanychannelinputs.Giventhatabinarydigitcanconveyamaximumofonebitofinformation,anoiselesschannelwhichcommunicatesRbinarydigitspersecondcancommunicateinformationattherateofuptoRbits/s.BecausethecapacityCisthemaximumrateatwhichitcancommunicateinformationfrominputtooutput,itfollowsthatthecapacityofanoiselesschannelisnumericallyequaltothenumberRofbinarydigitscommunicatedpersecond.However,ifeachbinarydigitcarrieslessthanonebit(e.g.ifconsecutiveoutputvaluesarecorrelated)thenthechannelcommunicatesinformationatalowerrateR<C.Nowthatwearefamiliarwiththecoreconceptsofinformationtheory,wecanquoteShannon\\xe2\\x80\\x99ssourcecodingtheoreminfull.ThisisalsoknownasShannon\\xe2\\x80\\x99sfundamentaltheoremforadiscretenoiselesschannel,andasthe\\xef\\xac\\x81rstfundamentalcodingtheorem.LetasourcehaveentropyH(bitspersymbol)andachannelhaveacapacityC(bitspersecond).ThenitispossibletoencodetheoutputofthesourceinsuchawayastotransmitattheaveragerateC/H\\xe2\\x88\\x92\\x01symbolspersecondoverthechannelwhere\\x01isarbitrarilysmall.ItisnotpossibletotransmitatanaveragerategreaterthanC/H[symbols/s].ShannonandWeaver,1949[2].[Textinsquarebracketshasbeenaddedbytheauthor.]Recallingtheexampleofthesumoftwodice,anaiveencodingwouldrequire3.46(log11)binarydigitstorepresentthesumofeachthrow.However,Shannon\\xe2\\x80\\x99ssourcecodingtheoremguaranteesthatanencodingexistssuchthatanaverageof(justover)3.27(i.e.log9.65)binarydigitspervalueofswillsu\\xef\\xac\\x83ce(thephrase\\xe2\\x80\\x98justover\\xe2\\x80\\x99isaninformalinterpretationofShannon\\xe2\\x80\\x99smoreprecisephrase\\xe2\\x80\\x98arbitrarilycloseto\\xe2\\x80\\x99).Thisencodingprocessyieldsinputswithaspeci\\xef\\xac\\x81cdistributionp(x),wherethereareimplicitconstraintsontheformofp(x)(e.g.powerconstraints).Theshapeofthedistributionp(x)placesanupperlimitontheentropyH(x),andthereforeonthemaximuminformationthateachinputcancarry.Thus,thecapacityofanoiselesschannelisde\\xef\\xac\\x81ned10\\x0cintermsoftheparticulardistributionp(x)whichmaximisestheamountofinformationperinputC=maxp(x)H(x)bitsperinput.(10)ThisstatesthatchannelcapacityCisachievedbythedistributionp(x)whichmakesH(x)aslargeaspossible(seeSection6).9NoiseReducesChannelCapacityHere,weexaminehownoisee\\xef\\xac\\x80ectivelyreducesthemaximuminformationthatachannelcancommunicate.Ifthenumberofequiprobable(signal)inputstatesismxthentheinputentropyisH(x)=logmxbits.(11)Forexample,supposetherearemx=3equiprobableinputstates,say,x1=100andx2=200andx3=300,sotheinputentropyisH(x)=log3=1.58bits.Andiftherearem\\xce\\xb7=2equiprobablevaluesforthechannelnoise,say,\\xce\\xb71=10and\\xce\\xb72=20,thenthenoiseentropyisH(\\xce\\xb7)=log2=1.00bit.Now,iftheinputisx1=100thentheoutputcanbeoneoftwoequiprobablestates,y1=100+10=110ory2=100+20=120.Andiftheinputisx2=200thenthe2H(y|x) 2H(x|y) Number\\tof\\t\\toutputs\\t=\\t2H(y) Number\\tof\\t\\tinputs\\t=\\t2H(x) Possible\\toutputs\\t\\tfor\\tone\\tinput\\t=\\t2H(y|x) Possible\\tinputs\\t\\tfor\\tone\\toutput\\t=\\t2H(x|y) Inputs\\tx Outputs\\ty Figure7:Afandiagramshowshowchannelnoisea\\xef\\xac\\x80ectsthenumberofpossibleoutputsgivenasingleinput,andviceversa.Ifthenoise\\xce\\xb7inthechanneloutputhasentropyH(\\xce\\xb7)=H(Y|X)theneachinputvaluecouldyieldoneof2H(Y|X)equallyprobableoutputvalues.Similarly,ifthenoiseinthechannelinputhasentropyH(X|Y)theneachoutputvaluecouldhavebeencausedbyoneof2H(X|Y)equallyprobableinputvalues.11\\x0coutputcanbeeithery3=210ory4=220.Finally,iftheinputisx3=300thentheoutputcanbeeithery5=310ory6=320.Thus,giventhreeequiprobableinputstatesandtwoequiprobablenoisevalues,therearemy=6(=3\\xc3\\x972)equiprobableoutputstates.SotheoutputentropyisH(y)=log6=2.58bits.However,someofthisentropyisduetonoise,sonotalloftheoutputentropycomprisesinformationabouttheinput.Ingeneral,thetotalnumbermyofequiprobableoutputstatesismy=mx\\xc3\\x97m\\xce\\xb7,fromwhichitfollowsthattheoutputentropyisH(y)=logmx+logm\\xce\\xb7(12)=H(x)+H(\\xce\\xb7)bits.(13)Becausewewanttoexplorechannelcapacityintermsofchannelnoise,wewillpretendtoreversethedirectionofdataalongthechannel.Accordingly,beforewe\\xe2\\x80\\x98receive\\xe2\\x80\\x99aninputvalue,weknowthattheoutputcanbeoneof6values,soouruncertaintyabouttheinputvalueissummarisedbyitsentropyH(y)=2.58bits.ConditionalEntropy.OuraverageuncertaintyabouttheoutputvaluegivenaninputvalueistheconditionalentropyH(y|x).Theverticalbardenotes\\xe2\\x80\\x98giventhat\\xe2\\x80\\x99,soH(y|x)is,\\xe2\\x80\\x98theresidualuncertainty(entropy)ofygiventhatweknowthevalueofx\\xe2\\x80\\x99.Afterwehavereceivedaninputvalue,ouruncertaintyabouttheoutputvalueisreducedfromH(y)=2.58bitstoH(y|x)=H(\\xce\\xb7)=log2=1bit.(14)Figure8:Therelationshipsbetweeninformationtheoreticquantities.Noisereferstonoise\\xce\\xb7intheoutput,whichinducesuncertaintyH(y|x)=H(\\xce\\xb7)regardingtheoutputgiventheinput;thisnoisealsoinducesuncertaintyH(x|y)regardingtheinputgiventheoutput.ThemutualinformationisI(x,y)=H(x)\\xe2\\x88\\x92H(x|y)=H(y)\\xe2\\x88\\x92H(y|x)bits.12\\x0cBecauseH(y|x)istheentropyofthechannelnoise\\xce\\xb7,wecanwriteitasH(\\xce\\xb7).Equation14istrueforeveryinput,anditisthereforetruefortheaverageinput.Thus,foreachinput,wegainanaverageofH(y)\\xe2\\x88\\x92H(\\xce\\xb7)=2.58\\xe2\\x88\\x921bits,(15)abouttheoutput,whichisthemutualinformationbetweenxandy.10MutualInformationThemutualinformationI(x,y)betweentwovariables,suchasachannelinputxandoutputy,istheaverageamountofinformationthateachvalueofxprovidesaboutyI(x,y)=H(y)\\xe2\\x88\\x92H(\\xce\\xb7)bits.(16)Somewhatcounter-intuitively,theaverageamountofinformationgainedabouttheoutputwhenaninputvalueisreceivedisthesameastheaverageamountofinformationgainedabouttheinputwhenanoutputvalueisreceived,I(x,y)=I(y,x).Thisiswhyitdidnotmatterwhenwepretendedtoreversethedirectionofdatathroughthechannel.ThesequantitiesaresummarisedinFigure8.11Shannon\\xe2\\x80\\x99sNoisyChannelCodingTheoremAllpracticalcommunicationchannelsarenoisy.Totakeatrivialexample,thevoicesignalcomingoutofatelephoneisnotaperfectcopyofthespeaker\\xe2\\x80\\x99svoicesignal,becausevariouselectricalcomponentsintroducespuriousbitsofnoiseintothetelephonesystem.Aswehaveseen,thee\\xef\\xac\\x80ectsofnoisecanbereducedbyusingerrorcorrectingcodes.Thesecodesreduceerrors,buttheyalsoreducetherateatwhichinformationiscommunicated.Moregenerally,anymethodwhichreducesthee\\xef\\xac\\x80ectsofnoisealsoreducestherateatwhichinformationcanbecommunicated.Takingthislineofreasoningtoitslogicalconclusionseemstoimplythattheonlywaytocommunicateinformationwithzeroerroristoreducethee\\xef\\xac\\x80ectiverateofinformationtransmissiontozero,andinShannon\\xe2\\x80\\x99sdaythiswaswidelybelievedtobetrue.ButShannonprovedthatinformationcanbecommunicated,withvanishinglysmallerror,ataratewhichislimitedonlybythechannelcapacity.NowwegiveShannon\\xe2\\x80\\x99sfundamentaltheoremforadiscretechannelwithnoise,alsoknownasthesecondfundamentalcodingtheorem,andasShannon\\xe2\\x80\\x99snoisychannelcodingtheorem[2]:13\\x0cLetadiscretechannelhavethecapacityCandadiscretesourcetheentropypersecondH.IfH\\xe2\\x89\\xa4Cthereexistsacodingsystemsuchthattheoutputofthesourcecanbetransmittedoverthechannelwithanarbitrarilysmallfrequencyoferrors(oranarbitrarilysmallequivocation).IfH\\xe2\\x89\\xa5CitispossibletoencodethesourcesothattheequivocationislessthanH\\xe2\\x88\\x92C+\\x01where\\x01isarbitrarilysmall.ThereisnomethodofencodingwhichgivesanequivocationlessthanH\\xe2\\x88\\x92C.(Theword\\xe2\\x80\\x98equivocation\\xe2\\x80\\x99meanstheaverageuncertaintythatremainsregardingthevalueoftheinputaftertheoutputisobserved,i.e.theconditionalentropyH(X|Y)).Inessence,Shannon\\xe2\\x80\\x99stheoremstatesthatitispossibletouseacommunicationchanneltocommunicateinformationwithalowerrorrate\\x01(epsilon),ataratearbitrarilyclosetothechannelcapacityofCbits/s,butitisnotpossibletocommunicateinformationatarategreaterthanCbits/s.Thecapacityofanoisychannelisde\\xef\\xac\\x81nedasC=maxp(x)I(x,y)(17)=maxp(x)[H(y)\\xe2\\x88\\x92H(y|x)]bits.(18)Ifthereisnonoise(i.e.ifH(y|x)=0)thenthisreducestoEquation10,whichisthecapacityofanoiselesschannel.Thedataprocessinginequalitystatesthat,nomatterhowsophisticatedanydeviceis,theamountofinformationI(x,y)initsoutputaboutitsinputcannotbegreaterthantheamountofinformationH(x)intheinput.12TheGaussianChannelIfthenoisevaluesinachannelaredrawnindependentlyfromaGaussiandistribution(i.e.\\xce\\xb7\\xe2\\x88\\xbcN(\\xc2\\xb5\\xce\\xb7,v\\xce\\xb7),asde\\xef\\xac\\x81nedinEquation6)thenthisde\\xef\\xac\\x81nesaGaussianchannel.Giventhaty=x+\\xce\\xb7,ifwewantp(y)tobeGaussianthenweshouldensurethatp(x)andp(\\xce\\xb7)areGaussian,becausethesumoftwoindependentGaussianvariablesisalsoGaussian[3].So,p(x)mustbe(iid)GaussianinordertomaximiseH(x),whichmaximisesH(y),whichmaximisesI(x,y).Thus,ifeachinput,output,andnoisevariableis(iid)Gaussianthentheaverageamountofinformationcommunicatedperoutputvalueisthechannelcapacity,sothatI(x,y)=Cbits.ThisisaninformalstatementofShannon\\xe2\\x80\\x99scontinuousnoisychannelcodingtheoremforGaussianchannels.Wecanusethistoexpresscapacityintermsoftheinput,output,andnoise.14\\x0cIfthechannelinputx\\xe2\\x88\\xbcN(\\xc2\\xb5x,vx)thenthecontinuousanalogue(integral)ofEquation4yieldsthedi\\xef\\xac\\x80erentialentropyH(x)=(1/2)log2\\xcf\\x80evxbits.(19)Thedistinctionbetweendi\\xef\\xac\\x80erentialentropye\\xef\\xac\\x80ectivelydisappearswhenconsideringthedi\\xef\\xac\\x80erencebetweenentropies,andwewilltherefore\\xef\\xac\\x81ndthatwecansafelyignorethisdistinctionhere.GiventhatthechannelnoiseisiidGaussian,itsentropyisH(\\xce\\xb7)=(1/2)log2\\xcf\\x80ev\\xce\\xb7bits.(20)Becausetheoutputisthesumy=x+\\xce\\xb7,itisalsoiidGaussianwithvariancevy=vx+v\\xce\\xb7,anditsentropyisH(y)=(1/2)log2\\xcf\\x80e(vx+v\\xce\\xb7)bits.(21)SubstitutingEquations20and21intoEquation16yieldsI(x,y)=12log(cid:18)1+vxv\\xce\\xb7(cid:19)bits,(22)whichallowsustochooseoneoutofm=2Iequiprobablevalues.ForaGaussianchannel,I(x,y)attainsitsmaximalvalueofCbits.Thevarianceofanysignalwithameanofzeroisequaltoitspower,whichistherateatwhichenergyisexpendedpersecond,andthephysicalunitofpowerismeasuredinJoulespersecond(J/s)orWatts,where1Watt=1J/s.Accordingly,thesignalpowerisS=vxJ/s,andthenoisepowerisN=v\\xce\\xb7J/s.ThisyieldsShannon\\xe2\\x80\\x99sfamousequationforthe05101520Signal power, J/s00.511.522.5Channel capacity, bits/sFigure9:GaussianchannelcapacityC(Equation23)increasesslowlywithsignalpowerS,whichequalssignalpowerherebecauseN=1.15\\x0ccapacityofaGaussianchannelC=12log(cid:18)1+SN(cid:19)bits,(23)wheretheratioofvariancesS/Nisthesignaltonoiseratio(SNR),asinFigure9.Itisworthnotingthat,givenaGaussiansignalobscuredbyGaussiannoise,theprobabilityofdetectingthesignalis[5]P=12log(cid:32)1+erf(cid:32)(cid:114)S8N(cid:33)(cid:33),(24)whereerfisthecumulativedistributionfunctionofaGaussian.13FourierAnalysisIfasinusoidalsignalhasaperiodof\\xce\\xbbsecondsthenithasafrequencyoff=1/\\xce\\xbbperiodspersecond,measuredinHertz(Hz).AsinusoidwithafrequencyofWHzcanberepresentedperfectlyifitsvalueismeasuredattheNyquistsamplerate[6]of2Wtimespersecond.Indeed,FourieranalysisallowsalmostanysignalxtoberepresentedasamixtureofsinusoidalFouriercomponentsx(f):(f=0,...,W),showninFigure10(seeSection13).Asignalwhichincludesfrequenciesbetween0HzandWHzhasabandwidthofWHz.FourierAnalysis.Fourieranalysisallowsanysignaltoberepresentedasaweightedsumofsineandcosinefunctions(seeSection13).Moreformally,considerasignalxwithavalueabcdFigure10:FourieranalysisdecomposesthesignalxindintoauniquesetofsinusoidalFouriercomponentsx(f)(f=0,...,WHz)ina-c,whered=a+b+c.16\\x0cxtattimet,whichspansatimeintervalofTseconds.Thissignalcanberepresentedasaweightedaverageofsineandcosinefunctionsxt=x0+\\xe2\\x88\\x9e(cid:88)n=1ancos(fnt)+\\xe2\\x88\\x9e(cid:88)n=1bnsin(fnt),(25)wherefn=2\\xcf\\x80n/Trepresentsfrequency,anistheFouriercoe\\xef\\xac\\x83cient(amplitude)ofacosinewithfrequencyfn,andbnistheFouriercoe\\xef\\xac\\x83cientofasinewithfrequencyfn;andx0representsthebackgroundamplitude(usuallyassumedtobezero).Takenoverallfrequencies,thesepairsofcoe\\xef\\xac\\x83cientsrepresenttheFouriertransformofx.TheFouriercoe\\xef\\xac\\x83cientscanbefoundfromtheintegralsan=2T(cid:90)T0xtcos(fnt)dt(26)bn=2T(cid:90)T0xtsin(fnt)dt.(27)Eachcoe\\xef\\xac\\x83cientanspeci\\xef\\xac\\x81eshowmuchofthesignalxconsistsofacosineatthefrequencyfn,andbnspeci\\xef\\xac\\x81eshowmuchconsistsofasine.Eachpairofcoe\\xef\\xac\\x83cientsspeci\\xef\\xac\\x81esthepowerandphaseofonefrequencycomponent;thepoweratfrequencyfnisSf=(a2n+b2n),andthephaseisarctan(bn/an).IfxhasabandwidthofWHzthenitspowerspectrumisthesetofWvaluesS0,...,SW.AnextremelyusefulpropertyofFourieranalysisisthat,whenappliedtoanyvariable,theresultantFouriercomponentsaremutuallyuncorrelated[7],and,whenappliedtoanyGaussianvariable,theseFouriercomponentsarealsomutuallyindependent.ThismeansthattheentropyofanyGaussianvariablecanbeestimatedbyaddinguptheentropiesofitsFouriercomponents,whichcanbeusedtoestimatethemutualinformationbetweenGaussianvariables.Consideravariabley=x+\\xce\\xb7,whichisthesumofaGaussiansignalxwithvarianceS,andGaussiannoisewithvarianceN.IfthehighestfrequencyinyisWHz,andifvaluesofxaretransmittedattheNyquistrateof2WHz,thenthechannelcapacityis2WCbitspersecond,(whereCisde\\xef\\xac\\x81nedinEquation23).Thus,whenexpressedintermsofbitspersecond,thisyieldsachannelcapacityofC=Wlog(cid:18)1+SN(cid:19)bits/s.(28)IfthesignalpowerofFouriercomponentx(f)isS(f),andthenoisepowerofcomponent\\xce\\xb7(f)isN(f)thenthesignaltonoiseratioisS(f)/N(f)(seeSection13).ThemutualinformationatfrequencyfisthereforeI(x(f),y(f))=log(cid:18)1+S(f)N(f)(cid:19)bits/s.(29)17\\x0cBecausetheFouriercomponentsofanyGaussianvariablearemutuallyindependent,themutualinformationbetweenGaussianvariablescanbeobtainedbysummingI(x(f),y(f))overfrequencyI(x,y)=(cid:90)Wf=0I(x(f),y(f))dfbits/s.(30)IfeachGaussianvariablex,yand\\xce\\xb7isalsoiidthenI(x,y)=Cbits/s,otherwiseI(x,y)<Cbits/s[2].IfthepeakpoweratallfrequenciesisaconstantkthenitcanbeshownthatI(x,y)ismaximisedwhenS(f)+N(f)=k,whichde\\xef\\xac\\x81nesa\\xef\\xac\\x82atpowerspectrum.Finally,ifthesignalspectrumissculptedsothatthesignalplusnoisespectrumis\\xef\\xac\\x82atthenthelogarithmicrelationinEquation23yieldsimproved,albeitstilldiminishing,returns[7]C\\xe2\\x88\\x9d(S/N)1/3bits/s.14AVeryShortHistoryofInformationTheoryEventhemostgiftedscientistcannotcommandanoriginaltheoryoutofthinair.JustasEinsteincouldnothavedevisedhistheoriesofrelativityifhehadnoknowledgeofNewton\\xe2\\x80\\x99swork,soShannoncouldnothavecreatedinformationtheoryifhehadnoknowledgeoftheworkofBoltzmann(1875)andGibbs(1902)onthermodynamicentropy,Wiener(1927)onsignalprocessing,Nyquist(1928)onsamplingtheory,orHartley(1928)oninformationtransmission[8].EventhoughShannonwasnotaloneintryingtosolveoneofthekeyscienti\\xef\\xac\\x81cproblemsofhistime(i.e.howtode\\xef\\xac\\x81neandmeasureinformation),hewasaloneinbeingabletoproduceacompletemathematicaltheoryofinformation:atheorythatmightotherwisehavetakendecadestoconstruct.Ine\\xef\\xac\\x80ect,Shannonsingle-handedlyacceleratedtherateofscienti\\xef\\xac\\x81cprogress,anditisentirelypossiblethat,withouthiscontribution,wewouldstillbetreatinginformationasifitweresomeill-de\\xef\\xac\\x81nedvital\\xef\\xac\\x82uid.15KeyEquationsLogarithmsusebase2unlessstatedotherwise.EntropyH(x)=m(cid:88)i=1p(xi)log1p(xi)bits(31)H(x)=(cid:90)xp(x)log1p(x)dxbits(32)18\\x0cJointentropyH(x,y)=m(cid:88)i=1m(cid:88)j=1p(xi,yj)log1p(xi,yj)bits(33)H(x,y)=(cid:90)x(cid:90)yp(y,x)log1p(y,x)dydxbits(34)H(x,y)=I(x,y)+H(x|y)+H(y|x)bits(35)ConditionalEntropyH(y|x)=m(cid:88)i=1m(cid:88)j=1p(xi,yj)log1p(xi|yj)bits(36)H(y|x)=m(cid:88)i=1m(cid:88)j=1p(xi,yj)log1p(yj|xi)bits(37)H(x|y)=(cid:90)y(cid:90)xp(x,y)log1p(x|y)dxdybits(38)H(y|x)=(cid:90)y(cid:90)xp(x,y)log1p(y|x)dxdybits(39)H(x|y)=H(x,y)\\xe2\\x88\\x92H(y)bits(40)H(y|x)=H(x,y)\\xe2\\x88\\x92H(x)bits(41)fromwhichweobtainthechainruleforentropyH(x,y)=H(x)+H(y|x)bits(42)=H(y)+H(x|y)bits(43)Marginalisationp(xi)=m(cid:88)j=1p(xi,yj),p(yj)=m(cid:88)i=1p(xi,yj)(44)p(x)=(cid:90)yp(x,y)dy,p(y)=(cid:90)xp(x,y)dx(45)19\\x0cMutualInformationI(x,y)=m(cid:88)i=1m(cid:88)j=1p(xi,yj)logp(xi,yj)p(xi)p(yj)bits(46)I(x,y)=(cid:90)y(cid:90)xp(x,y)logp(x,y)p(x)p(y)dxdybits(47)I(x,y)=H(x)+H(y)\\xe2\\x88\\x92H(x,y)(48)=H(x)\\xe2\\x88\\x92H(x|y)(49)=H(y)\\xe2\\x88\\x92H(y|x)(50)=H(x,y)\\xe2\\x88\\x92[H(x|y)+H(y|x)]bits(51)Ify=x+\\xce\\xb7,withxandy(notnecessarilyiid)GaussianvariablesthenI(x,y)=(cid:90)Wf=0log(cid:18)1+S(f)N(f)(cid:19)dfbits/s,(52)whereWisthebandwidth,S(f)/N(f)isthesignaltonoiseratioofthesignalandnoiseFouriercomponentsatfrequencyf(Section13),anddataaretransmittedattheNyquistrateof2Wsamples/s.ChannelCapacityC=maxp(x)I(x,y)bitspervalue.(53)IfthechannelinputxhasvarianceS,thenoise\\xce\\xb7hasvarianceN,andbothxand\\xce\\xb7areiidGaussianvariablesthenI(x,y)=C,whereC=12log(cid:18)1+SN(cid:19)bitspervalue,(54)wheretheratioofvariancesS/Nisthesignaltonoiseratio.FurtherReadingApplebaumD(2008)[9].ProbabilityandInformation:AnIntegratedApproach.Athoroughintroductiontoinformationtheory,whichstrikesagoodbalancebetweenintuitiveandtechnicalexplanations.AveryJ(2012)[10].InformationTheoryandEvolution.Anengagingaccountofhowinformationtheoryisrelevanttoawiderangeofnaturalandman-madesystems,including20\\x0cevolution,physics,cultureandgenetics.Includesinterestingbackgroundstoriesonthedevelopmentofideaswithinthesedi\\xef\\xac\\x80erentdisciplines.BaeyerHV(2005)[11].Information:TheNewLanguageofScienceErudite,wide-ranging,andinsightfulaccountofinformationtheory.Containsnoequations,whichmakesitveryreadable.CoverTandThomasJ(1991)[12].ElementsofInformationTheory.Comprehensive,andhighlytechnical,withhistoricalnotesandanequationsummaryattheendofeachchapter.GhahramaniZ(2002).InformationTheory.EncyclopediaofCognitiveScience.Anexcellent,briefoverviewofinformation.GleickJ(2012)[13].TheInformation.Aninformalintroductiontothehistoryofideasandpeopleassociatedwithinformationtheory.GuizzoEM(2003)[14].TheEssentialMessage:ClaudeShannonandtheMakingofInformationTheory.Master\\xe2\\x80\\x99sThesis,MassachusettsInstituteofTechnology.OneofthefewaccountsofShannon\\xe2\\x80\\x99sroleinthedevelopmentofinformationtheory.Seehttp://dspace.mit.edu/bitstream/handle/1721.1/39429/54526133.pdf.Laughlin,SB(2006).TheHungryEye:Energy,InformationandRetinalFunction,ExcellentlectureontheenergycostofShannoninformationineyes.Seehttp://www.crsltd.com/guest-talks/crs-guest-lecturers/simon-laughlin.MacKayDJC(2003)[15].InformationTheory,Inference,andLearningAlgorithms.Themodernclassiconinformationtheory.Averyreadabletextthatroamsfarandwideovermanytopics.Thebook\\xe2\\x80\\x99swebsite(below)alsohasalinktoanexcellentseriesofvideolecturesbyMacKay.Availablefreeonlineathttp://www.inference.phy.cam.ac.uk/mackay/itila/.PierceJR(1980)[8].AnIntroductiontoInformationTheory:Symbols,SignalsandNoise.SecondEdition.Piercewriteswithaninformal,tutorialstyleofwriting,butdoesnot\\xef\\xac\\x82inchfrompresentingthefundamentaltheoremsofinformationtheory.Thisbookprovidesagoodbalancebetweenwordsandequations.RezaFM(1961)[3].AnIntroductiontoInformationTheory.AmorecomprehensiveandmathematicallyrigorousbookthanPierce\\xe2\\x80\\x99sbook,itshouldbereadonlyafter\\xef\\xac\\x81rstreadingPierce\\xe2\\x80\\x99smoreinformaltext.SeifeC(2007)[16].DecodingtheUniverse:HowtheNewScienceofInformationIsExplainingEverythingintheCosmos,FromOurBrainstoBlackHoles.Alucidand21\\x0cengagingaccountoftherelationshipbetweeninformation,thermodynamicentropyandquantumcomputing.Highlyrecommended.ShannonCEandWeaverW(1949)[2].TheMathematicalTheoryofCommunication.UniversityofIllinoisPress.Asurprisinglyaccessiblebook,writteninanerawheninformationtheorywasknownonlytoaprivilegedfew.Thisbookcanbedownloadedfromhttp://cm.bell-labs.com/cm/ms/what/shannonday/paper.htmlSoni,JandGoodman,R(2017)[17].Amindatplay:HowClaudeShannoninventedtheinformationageAbiographyofShannon.Stone,JV(2015)[4]InformationTheory:ATutorialIntroduction.Amoreextensiveintroductionthanthecurrentarticle.Forthecompletenovice,thevideosattheonlineKahnAcademyprovideanexcellentintroduction.Additionally,theonlineScholarpediawebpagebyLathamandRudiprovidesalucidtechnicalaccountofmutualinformation:http://www.scholarpedia.org/article/Mutual_information.Finally,somehistoricalperspectiveisprovidedinalonginterviewwithShannonconductedin1982:http://www.ieeeghn.org/wiki/index.php/Oral-History:Claude_E._Shannon.References[1]CEShannon.Amathematicaltheoryofcommunication.BellSystemTechnicalJournal,27:379\\xe2\\x80\\x93423,1948.[2]CEShannonandWWeaver.TheMathematicalTheoryofCommunication.UniversityofIllinoisPress,1949.[3]FMReza.InformationTheory.NewYork,McGraw-Hill,1961.[4]JVStone.InformationTheory:ATutorialIntroduction.SebtelPress,She\\xef\\xac\\x83eld,England,2015.[5]SRSchultz.Signal-to-noiseratioinneuroscience.Scholarpedia,2(6):2046,2007.[6]H.Nyquist.Certaintopicsintelegraphtransmissiontheory.ProceedingsoftheIEEE,90(2):280\\xe2\\x80\\x93305,1928.[7]FRieke,DWarland,RRdeRuytervanSteveninck,andWBialek.Spikes:ExploringtheNeuralCode.MITPress,Cambridge,MA,1997.22\\x0c[8]JRPierce.Anintroductiontoinformationtheory:Symbols,signalsandnoise.Dover,1980.[9]DApplebaum.ProbabilityandInformationAnIntegratedApproach,2ndEdition.CambridgeUniversityPress,Cambridge,2008.[10]JAvery.InformationTheoryandEvolution.WorldScienti\\xef\\xac\\x81cPublishing,2012.[11]HVBaeyer.Information:TheNewLanguageofScience.HarvardUniversityPress,2005.[12]TMCoverandJAThomas.ElementsofInformationTheory.NewYork,JohnWileyandSons,1991.[13]JGleick.TheInformation.Vintage,2012.[14]EMGuizzo.Theessentialmessage:ClaudeShannonandthemakingofinformationtheory.http://dspace.mit.edu/bitstream/handle/1721.1/39429/54526133.pdf.Mas-sachusettsInstituteofTechnology,2003.[15]DJCMacKay.Informationtheory,inference,andlearningalgorithms.CambridgeUniversityPress,2003.[16]CSeife.DecodingtheUniverse:HowtheNewScienceofInformationIsExplainingEverythingintheCosmos,FromOurBrainstoBlackHoles.Penguin,2007.[17]JSoniandRGoodman.Amindatplay:HowClaudeShannoninventedtheinformationage.SimonandSchuster,2017.23\\x0c'\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic model part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/ellen/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to /Users/ellen/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.load('en')\n",
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "\n",
    "\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)\n",
    "\n",
    "\n",
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['informationtheory',\n 'atutorialintroductionjamesvstone',\n 'psychologydepartment',\n 'universityofsheﬃeld',\n 'england.j.v.stone@sheﬃeld.ac.ukfile',\n 'maininformationtheoryjvstonev3.texabstractshannon’smathematicaltheoryofcommunicationdeﬁnesfundamentallimitsonhowmuchinformationcanbetransmittedbetweenthediﬀerentcomponentsofanyman',\n 'madeorbiologicalsystem',\n 'thispaperisaninformalbutrigorousintroductiontothemainideasimplicitinshannon’stheory',\n 'anannotatedreadinglistisprovidedforfurtherreading.1introductionin1948,claudeshannonpublishedapapercalledamathematicaltheoryofcommunication[1].thispaperheraldedatransformationinourunderstandingofinformation',\n 'beforeshannon’spaper',\n 'informationhadbeenviewedasakindofpoorlydeﬁnedmiasmicﬂuid',\n 'butaftershannon’spaper',\n 'itbecameapparentthatinformationisawell',\n 'deﬁnedand',\n 'aboveall',\n 'measurablequantity',\n 'indeed',\n 'asnotedbyshannon',\n 'abasicideaininformationtheoryisthatinformationcanbetreatedverymuchlikeaphysicalquantity',\n 'suchasmassorenergy',\n 'caudeshannon,1985.encodingmessageschannelnoisedecodingηxymessagesfigure1',\n 'thecommunicationchannel',\n 'amessage(data)isencodedbeforebeingusedasinputtoacommunicationchannel',\n 'whichaddsnoise',\n 'thechanneloutputisdecodedbyareceivertorecoverthemessage.1arxiv:1802.05968v2',\n 'informationtheorydeﬁnesdeﬁnite',\n 'unbreachablelimitsonpreciselyhowmuchinformationcanbecommunicatedbetweenanytwocomponentsofanysystem',\n 'whetherthissystemisman',\n 'madeornatural',\n 'thetheoremsofinformationtheoryaresoimportantthattheydeservetoberegardedasthelawsofinformation[2,3,4].thebasiclawsofinformationcanbesummarisedasfollows',\n 'foranycommunicationchannel(figure1):1)thereisadeﬁniteupperlimit',\n 'thechannelcapacity',\n 'totheamountofinformationthatcanbecommunicatedthroughthatchannel,2)thislimitshrinksastheamountofnoiseinthechannelincreases,3)thislimitcanverynearlybereachedbyjudiciouspackaging',\n 'orencoding',\n 'ofdata.2findingaroute',\n 'bitbybitinformationisusuallymeasuredinbits',\n 'andonebitofinformationallowsyoutochoosebetweentwoequallyprobable',\n 'orequiprobable',\n 'alternative',\n 'inordertounderstandwhythisisso',\n 'imagineyouarestandingattheforkintheroadatpointainfigure2,andthatyouwanttogettothepointmarkedd.theforkatarepresentstwoequiprobablealternatives',\n 'soifitellyoutogoleftthenyouhavereceivedonebitofinformation',\n 'ifwerepresentmyinstructionwithabinarydigit(0',\n 'leftand1',\n 'right)thenthisbinarydigitprovidesyouwithonebitofinformation',\n 'whichtellsyouwhichroadtochoose',\n 'nowimaginethatyoucometoanotherfork',\n 'atpointbinfigure2.again',\n 'abinarydigit(1',\n 'right)providesonebitofinformation',\n 'allowingyoutochoosethecorrectroad',\n 'whichleadstoc.notethatcisoneoffourpossibleinterimdestinationsthatyoucouldabd0',\n '7c---------------',\n '----------------------leftright01001111110000figure2',\n 'foratravellerwhodoesnotknowtheway',\n 'eachforkintheroadrequiresonebitofinformationtomakeacorrectdecision',\n 'the0sand1sontheright',\n 'handsidesummarisetheinstructionsneededtoarriveateachdestination;aleftturnisindicatedbya0andarightturnbya1.2',\n 'havereachedaftermakingtwodecisions',\n 'thetwobinarydigitsthatallowyoutomakethecorrectdecisionsprovidedtwobitsofinformation',\n 'allowingyoutochoosefromfour(equiprobable)alternatives;4equals2×2=22.athirdbinarydigit(1',\n 'right)providesyouwithonemorebitofinformation',\n 'whichallowsyoutoagainchoosethecorrectroad',\n 'leadingtothepointmarkedd.therearenoweightroadsyoucouldhavechosenfromwhenyoustartedata',\n 'sothreebinarydigits(whichprovideyouwiththreebitsofinformation)allowyoutochoosefromeightequiprobablealternatives',\n 'whichalsoequals2×2×2=23=8.wecanrestatethisinmoregeneraltermsifweusentorepresentthenumberofforks',\n 'andmtorepresentthenumberofﬁnaldestinations',\n 'ifyouhavecometonforksthenyouhaveeﬀectivelychosenfromm=2nﬁnaldestinations',\n 'becausethedecisionateachforkrequiresonebitofinformation',\n 'nforksrequirenbitsofinformation',\n 'viewedfromanotherperspective',\n 'iftherearem=8possibledestinationsthenthenumberofforksisn=3,whichisthelogarithmof8.thus,3',\n 'log28isthenumberofforksimpliedbyeightdestinations',\n 'moregenerally',\n 'thelogarithmofmisthepowertowhich2mustberaisedinordertoobtainm;thatis',\n 'equivalently',\n 'givenanumberm',\n 'whichwewishtoexpressasalogarithm',\n 'log2m',\n 'thesubscript2indicatesthatweareusinglogstothebase2(alllogarithmsinthisbookusebase2unlessstatedotherwise).3bitsarenotbinarydigitsthewordbitisderivedfrombinarydigit',\n 'butabitandabinarydigitarefundamentallydiﬀerenttypesofquantities',\n 'abinarydigitisthevalueofabinaryvariable',\n 'whereasabitisanamountofinformation',\n 'tomistakeabinarydigitforabitisacategoryerror',\n 'inthiscase',\n 'thecategoryerrorisnotasbadasmistakingmarzipanforjustice',\n 'butitisanalogoustomistakingapint',\n 'sizedbottleforapintofmilk',\n 'justasabottlecancontainbetweenzeroandonepint',\n 'soabinarydigit(whenaveragedoverbothofitspossiblestates)canconveybetweenzeroandonebitofinformation.4informationandentropyconsideracoinwhichlandsheadsup90%ofthetime(i.e.p(xh)=0.9).whenthiscoinisﬂipped',\n 'weexpectittolandheadsup(x',\n 'xh),sowhenitdoessowearelesssurprisedthanwhenitlandstailsup(x',\n 'xt).themoreimprobableaparticularoutcomeis',\n 'themoresurprisedwearetoobserveit',\n 'ifweuselogarithmstothebase2thentheshannon3',\n 'informationorsurprisalofeachoutcome',\n 'suchasaheadxh',\n 'ismeasuredinbits(seefigure3a)shannoninformation',\n 'log1p(xh)bits,(1)whichisoftenexpressedas',\n 'information=−logp(xh)bits',\n 'entropyisaverageshannoninformation',\n 'wecanrepresenttheoutcomeofacoinﬂipastherandomvariablex',\n 'suchthataheadisx',\n 'xhandatailisx',\n 'inpractice',\n 'wearenotusuallyinterestedinthesurpriseofaparticularvalueofarandomvariable',\n 'butweareinterestedinhowmuchsurprise',\n 'onaverage',\n 'isassociatedwiththeentiresetofpossiblevalues',\n 'theaveragesurpriseofavariablexisdeﬁnedbyitsprobabilitydistributionp(x),andiscalledtheentropyofp(x),representedash(x).theentropyofafaircoin',\n 'theaverageamountofsurpriseaboutthepossibleoutcomesofacoinﬂipcanbefoundasfollows',\n 'ifacoinisfairorunbiasedthenp(xh)=p(xt)=0.5thentheshannoninformationgainedwhenaheadoratailisobservedislog1/0.5=1bit',\n 'sotheaverageshannoninformationgainedaftereachcoinﬂipisalso1bit',\n 'becauseentropyisdeﬁnedasaverageshannoninformation',\n 'theentropyofafaircoinish(x)=1bit',\n 'theentropyofanunfair(biased)coin',\n 'ifacoinisbiasedsuchthattheprobabilityofaheadisp(xh)=0.9thenitiseasytopredicttheresultofeachcoinﬂip(i.e.with90%accuracyifwepredictaheadforeachﬂip).iftheoutcomeisaheadthentheamountofshannoninformationgainedislog(1/0.9)=0.15bits',\n 'butiftheoutcomeisatailthen00.20.40.60.81',\n 'p(x)01234567surprise',\n 'p(x)(a)(b)figure3',\n 'a)shannoninformationassurprise',\n 'valuesofxthatarelessprobablehavelargervaluesofsurprise',\n 'deﬁnedaslog2(1',\n 'p(x))bits.b)graphofentropyh(x)versuscoinbias(probabilityp(xh)ofahead).theentropyofacoinistheaverageamountofsurpriseorshannoninformationinthedistributionofpossibleoutcomes(i.e.headsandtails).4',\n 'theamountofshannoninformationgainedislog(1/0.1)=3.32bits',\n 'noticethatmoreinformationisassociatedwiththemoresurprisingoutcome',\n 'giventhattheproportionofﬂipsthatyieldaheadisp(xh),andthattheproportionofﬂipsthatyieldatailisp(xt)(wherep(xh)+p(xt)=1),theaveragesurpriseish(x)=p(xh)log1p(xh)+p(xt)log1p(xt),(2)whichcomestoh(x)=0.469bits',\n 'asinfigure3b',\n 'ifwedeﬁneatailasx1',\n 'xtandaheadasx2',\n 'xhthenequation2canbewrittenash(x)=2(cid:88)i=1p(xi)log1p(xi)bits.(3)moregenerally',\n 'arandomvariablexwithaprobabilitydistributionp(x)={p(x1),',\n ',p(xm)}hasanentropyofh(x)=m(cid:88)i=1p(xi)log1p(xi)bits.(4)thereasonthisdeﬁnitionmattersisbecauseshannon’ssourcecodingtheorem(seesection7)guaranteesthateachvalueofthevariablexcanberepresentedwithanaverageof(justover)h(x)binarydigits',\n 'however',\n 'ifthevaluesofconsecutivevaluesofarandomvariablearenotindependenttheneachvalueismorepredictable',\n 'andthereforelesssurprising',\n 'whichreducestheinformation',\n 'carryingcapability(i.e.entropy)ofthevariable',\n 'thisiswhyitisimportanttospecifywhetherornotconsecutivevariablevaluesareindependent',\n 'interpretingentropy',\n 'ifh(x)=1bitthenthevariablexcouldbeusedtorepresentm=2h(x)or2equiprobablevalues',\n 'similarly',\n 'ifh(x)=0.469bitsthenthevariablex(a)2345678910111200.020.040.060.080.10.120.140.160.18outcome',\n 'valueoutcome',\n 'probability(b)figure4:(a)apairofdice.(b)histogramofdiceoutcomevalues.5',\n 'couldbeusedtorepresentm=20.469or1.38equiprobablevalues;asifwehadadiewith1.38sides',\n 'atﬁrstsight',\n 'thisseemslikeanoddstatement',\n 'nevertheless',\n 'translatingentropyintoanequivalentnumberofequiprobablevaluesservesasanintuitiveguidefortheamountofinformationrepresentedbyavariable',\n 'dicingwithentropy',\n 'throwingapairof6-sideddiceyieldsanoutcomeintheformofanorderedpairofnumbers',\n 'andthereareatotalof36equiprobableoutcomes',\n 'asshownintable1.ifwedeﬁneanoutcomevalueasthesumofthispairofnumbersthentherearem=11possibleoutcomevaluesax={2,3,4,5,6,7,8,9,10,11,12},representedbythesymbolsx1,',\n ',x11.theseoutcomevaluesoccurwiththefrequenciesshowninfigure4bandtable1.dividingthefrequencyofeachoutcomevalueby36yieldstheprobabilitypofeachoutcomevalue',\n 'usingequation4,wecanusethese11probabilitiestoﬁndtheentropyh(x)=p(x1)log1p(x1)+p(x2)log1p(x2)+···+p(x11)log1p(x11)=3.27bits',\n 'usingtheinterpretationdescribedabove',\n 'avariablewithanentropyof3.27bitscanrepresent23.27=9.65equiprobablevalues',\n 'entropyanduncertainty',\n 'entropyisameasureofuncertainty',\n 'whenouruncertaintyisreduced',\n 'wegaininformation',\n 'soinformationandentropyaretwosidesofthesamecoin',\n 'however',\n 'informationhasarathersubtleinterpretation',\n 'whichcaneasilyleadtoconfusion',\n 'averageinformationsharesthesamedeﬁnitionasentropy',\n 'butwhetherwecallagivenquantityinformationorentropyusuallydependsonwhetheritisbeinggiventousortakensymbolsumoutcomefrequencypsurprisalx121:110.035.17x231:2,2:120.064.17x341:3,3:1,2:230.083.59x452:3,3:2,1:4,4:140.113.17x562:4,4:2,1:5,5:1,3:350.142.85x673:4,4:3,2:5,5:2,1:6,6:160.172.59x783:5,5:3,2:6,6:2,4:450.142.85x893:6,6:3,4:5,5:440.113.17x9104:6,6:4,5:530.083.59x10115:6,6:520.064.17x11126:610.035.17table1',\n 'apairofdicehave36possibleoutcomes',\n 'outcomevalue',\n 'totalnumberofdotsforagiventhrowofthedice',\n 'outcome',\n 'orderedpairofdicenumbersthatcouldgenerateeachsymbol',\n 'numberofdiﬀerentoutcomesthatcouldgenerateeachoutcomevalue',\n 'theprobabilitythatthepairofdiceyieldagivenoutcomevalue(freq/36).surprisal',\n 'plog(1',\n 'p)bits.6',\n 'forexample',\n 'ifavariablehashighentropythenourinitialuncertaintyaboutthevalueofthatvariableislargeandis',\n 'bydeﬁnition',\n 'exactlyequaltoitsentropy',\n 'ifwearetoldthevalueofthatvariablethen',\n 'onaverage',\n 'wehavebeengivenanamountofinformationequaltotheuncertainty(entropy)weinitiallyhadaboutitsvalue',\n 'receivinganamountofinformationisequivalenttohavingexactlythesameamountofentropy(uncertainty)takenaway.5entropyofcontinuousvariablesfordiscretevariables',\n 'entropyiswell',\n 'deﬁned',\n 'however',\n 'forallcontinuousvariables',\n 'entropyiseﬀectivelyinﬁnite',\n 'considerthediﬀerencebetweenadiscretevariablexdwithnpossiblevaluesandacontinuousvariablexcwithanuncountablyinﬁnitenumberofpossiblevalues;forsimplicity',\n 'assumethatallvaluesareequallyprobable',\n 'theprobabilityofobservingeachvalueofthediscretevariableispd=1',\n 'sotheentropyofxdish(xd)=logm',\n 'incontrast',\n 'theprobabilityofobservingeachvalueofthecontinuousvariableispc=1/∞=0,sotheentropyofxcish(xc)=log∞=∞.inonerespect',\n 'thismakessense',\n 'becauseeachvalueofofacontinuousvariableisimplicitlyspeciﬁedwithinﬁniteprecision',\n 'fromwhichitfollowsthattheamountofinformationconveyedbyeachvalueisinﬁnite',\n 'however',\n 'thisresultimpliesthatallcontinuousvariableshavethesameentropy',\n 'inordertoassigndiﬀerentvaluestodiﬀerentvariables',\n 'allinﬁnitetermsaresimplyignored',\n 'whichyieldsthediﬀerentialentropyh(xc)=(cid:90)p(xc)log1p(xc)dxc.(5)itisworthnotingthatthetechnicaldiﬃcultiesassociatedwithentropyofcontinuousvariablesdisappearforquantitieslikemutualinformation',\n 'whichinvolvethediﬀerencebetweentwoentropies',\n 'forconvenience',\n 'weusethetermentropyforbothcontinuousanddiscretevariablesbelow.6maximumentropydistributionsadistributionofvaluesthathasasmuchentropy(information)astheoreticallypossibleisamaximumentropydistribution',\n 'maximumentropydistributionsareimportantbecause',\n 'ifwewishtouseavariabletotransmitasmuchinformationaspossiblethenwehadbettermakesureithasmaximumentropy',\n 'foragivenvariable',\n 'thepreciseformofitsmaximumentropydistributiondependsontheconstraintsplacedonthevaluesofthatvariable[3].itwillproveusefultosummarisethreeimportantmaximumentropydistributions',\n 'thesearelistedinorderofdecreasingnumbersofconstraintsbelow.7',\n 'thegaussiandistribution',\n 'ifavariablexhasaﬁxedvariance',\n 'butisotherwiseunconstrained',\n 'thenthemaximumentropydistributionisthegaussiandistribution(figure5a).thisisparticularlyimportantintermsofenergyeﬃciencybecausenootherdistributioncanprovideasmuchinformationatalowerenergycostperbit',\n 'ifavariablehasagaussianornormaldistributionthentheprobabilityofobservingaparticularvaluexisp(x)=1√2πvxe−(µx−x)2/(2vx),(6)wheree=2.7183.thisequationdeﬁnesthebell',\n 'shapedcurveinfigure5a',\n 'thetermµxisthemeanofthevariablex',\n 'anddeﬁnesthecentralvalueofthedistribution;weassumethatallvariableshaveameanofzero(unlessstatedotherwise).thetermvxisthevarianceofthevariablex',\n 'whichisthesquareofthestandarddeviationσxofx',\n 'anddeﬁnesthewidthofthebellcurve',\n 'equation6isaprobabilitydensityfunction',\n 'and(strictlyspeaking)p(x)istheprobabilitydensityofx',\n 'theexponentialdistribution',\n 'ifavariablehasnovaluesbelowzero',\n 'andhasaﬁxedmeanµ,butisotherwiseunconstrained',\n 'thenthemaximumentropydistributionisexponential',\n 'p(x)=1µe−x/µ,(7)whichhasavarianceofvar(x)=µ2,asshowninfigure5b',\n 'theuniformdistribution',\n 'ifavariablehasaﬁxedlowerboundxminandupperboundxmax',\n 'butisotherwiseunconstrained',\n 'thenthemaximumentropydistributionisuniform',\n 'p(x)=1/(xmax−xmin),(8)-3',\n '1012300.050.10.150.20.250.30.350.4p(x)xσ(a)01234500.10.20.30.40.50.60.70.80.91p(x)x(b)-0.500.511.522.500.10.20.30.40.5p(x)x(c)figure5',\n 'maximumentropydistributions.a)gaussiandistribution',\n 'withmeanµ=0andastandarddeviationσ=1(equation6).b)exponentialdistribution',\n 'withmeanindicatedbytheverticalline(equation7).c)auniformdistributionwitharangebetweenzeroandtwo(equation8).8',\n 'whichhasavariance(xmax−xmin)2/12,asshowninfigure5c.7channelcapacityaveryimportant(andconvenient)channelistheadditivechannel',\n 'asencodedvaluespassthroughanadditivechannel',\n 'noiseη(eta)isadded',\n 'sothatthechanneloutputisanoisyversionyofthechannelinputxy',\n 'x+η.(9)thechannelcapacitycisthemaximumamountofinformationthatachannelcanprovideatitsoutputabouttheinput',\n 'therateatwhichinformationistransmittedthroughthechanneldependsontheentropiesofthreevariables:1)theentropyh(x)oftheinput,2)theentropyh(y)oftheoutput,3)theentropyh(η)ofthenoiseinthechannel',\n 'iftheoutputentropyishighthenthisprovidesalargepotentialforinformationtransmission',\n 'andtheextenttowhichthispotentialisrealiseddependsontheinputentropyandthelevelofnoise',\n 'ifthenoiseislowthentheoutputentropycanbeclosetothechannelcapacity',\n 'however',\n 'channelcapacitygetsprogressivelysmallerasthenoiseincreases',\n 'capacityisusuallyexpressedinbitsperusage(i.e.bitsperoutput),orbitspersecond(bits',\n 's).channel',\n 'capacity',\n 'channel',\n 'capacity',\n 'binary',\n 'digit',\n 'transmit',\n 'noise',\n 'noisy',\n 'channel',\n 'noiseless',\n 'channel',\n 'figure6',\n 'thechannelcapacityofnoiselessandnoisychannelsisthemaximumrateatwhichinformationcanbecommunicated',\n 'ifanoiselesschannelcommunicatesdataat10binarydigits',\n 'sthenitscapacityisc=10bits',\n 'thecapacityofanoiselesschannelisnumericallyequaltotherateatwhichitcommunicatesbinarydigits',\n 'whereasthecapacityofanoisychannelislessthanthisbecauseitislimitedbytheamountofnoiseinthechannel.9',\n '8shannon’ssourcecodingtheoremshannon’ssourcecodingtheorem',\n 'describedbelow',\n 'appliesonlytonoiselesschannels',\n 'thistheoremisreallyaboutre',\n 'packaging(encoding)databeforeitistransmitted',\n 'sothat',\n 'whenitistransmitted',\n 'everydatumconveysasmuchinformationaspossible',\n 'thistheoremishighlyrelevanttothebiologicalinformationprocessingbecauseitdeﬁnesdeﬁnitelimitstohoweﬃcientlysensorydatacanbere',\n 'package',\n 'weconsiderthesourcecodingtheoremusingbinarydigitsbelow',\n 'butthelogicoftheargumentappliesequallywelltoanychannelinputs',\n 'giventhatabinarydigitcanconveyamaximumofonebitofinformation',\n 'anoiselesschannelwhichcommunicatesrbinarydigitspersecondcancommunicateinformationattherateofuptorbits',\n 'becausethecapacitycisthemaximumrateatwhichitcancommunicateinformationfrominputtooutput',\n 'itfollowsthatthecapacityofanoiselesschannelisnumericallyequaltothenumberrofbinarydigitscommunicatedpersecond',\n 'however',\n 'ifeachbinarydigitcarrieslessthanonebit(e.g.ifconsecutiveoutputvaluesarecorrelated)thenthechannelcommunicatesinformationatalowerrater',\n 'c.nowthatwearefamiliarwiththecoreconceptsofinformationtheory',\n 'wecanquoteshannon’ssourcecodingtheoreminfull',\n 'thisisalsoknownasshannon’sfundamentaltheoremforadiscretenoiselesschannel',\n 'andastheﬁrstfundamentalcodingtheorem',\n 'letasourcehaveentropyh(bitspersymbol)andachannelhaveacapacityc(bitspersecond).thenitispossibletoencodetheoutputofthesourceinsuchawayastotransmitattheaverageratec',\n 'h−\\x01symbolspersecondoverthechannelwhere\\x01isarbitrarilysmall',\n 'itisnotpossibletotransmitatanaveragerategreaterthanc',\n 'h[symbols',\n 's].shannonandweaver,1949[2].[textinsquarebracketshasbeenaddedbytheauthor.]recallingtheexampleofthesumoftwodice',\n 'anaiveencodingwouldrequire3.46(log11)binarydigitstorepresentthesumofeachthrow',\n 'however',\n 'shannon’ssourcecodingtheoremguaranteesthatanencodingexistssuchthatanaverageof(justover)3.27(i.e.log9.65)binarydigitspervalueofswillsuﬃce(thephrase‘justover’isaninformalinterpretationofshannon’smoreprecisephrase‘arbitrarilycloseto’).thisencodingprocessyieldsinputswithaspeciﬁcdistributionp(x),wherethereareimplicitconstraintsontheformofp(x)(e.g.powerconstraints).theshapeofthedistributionp(x)placesanupperlimitontheentropyh(x),andthereforeonthemaximuminformationthateachinputcancarry',\n 'thecapacityofanoiselesschannelisdeﬁned10',\n 'intermsoftheparticulardistributionp(x)whichmaximisestheamountofinformationperinputc',\n 'maxp(x)h(x)bitsperinput.(10)thisstatesthatchannelcapacitycisachievedbythedistributionp(x)whichmakesh(x)aslargeaspossible(seesection6).9noisereduceschannelcapacityhere',\n 'weexaminehownoiseeﬀectivelyreducesthemaximuminformationthatachannelcancommunicate',\n 'ifthenumberofequiprobable(signal)inputstatesismxthentheinputentropyish(x)=logmxbits.(11)forexample',\n 'supposetherearemx=3equiprobableinputstates',\n 'x1=100andx2=200andx3=300,sotheinputentropyish(x)=log3=1.58bits',\n 'andiftherearemη=2equiprobablevaluesforthechannelnoise',\n 'η1=10andη2=20,thenthenoiseentropyish(η)=log2=1.00bit',\n 'iftheinputisx1=100thentheoutputcanbeoneoftwoequiprobablestates',\n 'y1=100',\n '10=110ory2=100',\n '20=120.andiftheinputisx2=200thenthe2h(y|x',\n '2h(x|y',\n 'number',\n 'output',\n 'number',\n 'input',\n 'possible',\n 'output',\n 'input',\n '2h(y|x',\n 'possible',\n 'input',\n 'output',\n '2h(x|y',\n 'input',\n 'output',\n 'figure7',\n 'afandiagramshowshowchannelnoiseaﬀectsthenumberofpossibleoutputsgivenasingleinput',\n 'andviceversa',\n 'ifthenoiseηinthechanneloutputhasentropyh(η)=h(y|x)theneachinputvaluecouldyieldoneof2h(y|x)equallyprobableoutputvalues',\n 'similarly',\n 'ifthenoiseinthechannelinputhasentropyh(x|y)theneachoutputvaluecouldhavebeencausedbyoneof2h(x|y)equallyprobableinputvalues.11',\n 'outputcanbeeithery3=210ory4=220.finally',\n 'iftheinputisx3=300thentheoutputcanbeeithery5=310ory6=320.thus',\n 'giventhreeequiprobableinputstatesandtwoequiprobablenoisevalues',\n 'therearemy=6(=3×2)equiprobableoutputstates',\n 'sotheoutputentropyish(y)=log6=2.58bits',\n 'however',\n 'someofthisentropyisduetonoise',\n 'sonotalloftheoutputentropycomprisesinformationabouttheinput',\n 'ingeneral',\n 'thetotalnumbermyofequiprobableoutputstatesismy',\n 'mx×mη',\n 'fromwhichitfollowsthattheoutputentropyish(y)=logmx+logmη(12)=h(x)+h(η)bits.(13)becausewewanttoexplorechannelcapacityintermsofchannelnoise',\n 'wewillpretendtoreversethedirectionofdataalongthechannel',\n 'accordingly',\n 'beforewe‘receive’aninputvalue',\n 'weknowthattheoutputcanbeoneof6values',\n 'soouruncertaintyabouttheinputvalueissummarisedbyitsentropyh(y)=2.58bits',\n 'conditionalentropy',\n 'ouraverageuncertaintyabouttheoutputvaluegivenaninputvalueistheconditionalentropyh(y|x).theverticalbardenotes‘giventhat’,soh(y|x)is,‘theresidualuncertainty(entropy)ofygiventhatweknowthevalueofx’',\n 'afterwehavereceivedaninputvalue',\n 'ouruncertaintyabouttheoutputvalueisreducedfromh(y)=2.58bitstoh(y|x)=h(η)=log2=1bit.(14)figure8',\n 'therelationshipsbetweeninformationtheoreticquantities',\n 'noisereferstonoiseηintheoutput',\n 'whichinducesuncertaintyh(y|x)=h(η)regardingtheoutputgiventheinput;thisnoisealsoinducesuncertaintyh(x|y)regardingtheinputgiventheoutput',\n 'themutualinformationisi(x',\n 'y)=h(x)−h(x|y)=h(y)−h(y|x)bits.12',\n 'becauseh(y|x)istheentropyofthechannelnoiseη',\n 'wecanwriteitash(η).equation14istrueforeveryinput',\n 'anditisthereforetruefortheaverageinput',\n 'foreachinput',\n 'wegainanaverageofh(y)−h(η)=2.58−1bits,(15)abouttheoutput',\n 'whichisthemutualinformationbetweenxandy.10mutualinformationthemutualinformationi(x',\n 'y)betweentwovariables',\n 'suchasachannelinputxandoutputy',\n 'istheaverageamountofinformationthateachvalueofxprovidesaboutyi(x',\n 'y)=h(y)−h(η)bits.(16)somewhatcounter',\n 'intuitively',\n 'theaverageamountofinformationgainedabouttheoutputwhenaninputvalueisreceivedisthesameastheaverageamountofinformationgainedabouttheinputwhenanoutputvalueisreceived',\n 'y)=i(y',\n 'x).thisiswhyitdidnotmatterwhenwepretendedtoreversethedirectionofdatathroughthechannel',\n 'thesequantitiesaresummarisedinfigure8.11shannon’snoisychannelcodingtheoremallpracticalcommunicationchannelsarenoisy',\n 'totakeatrivialexample',\n 'thevoicesignalcomingoutofatelephoneisnotaperfectcopyofthespeaker’svoicesignal',\n 'becausevariouselectricalcomponentsintroducespuriousbitsofnoiseintothetelephonesystem',\n 'aswehaveseen',\n 'theeﬀectsofnoisecanbereducedbyusingerrorcorrectingcodes',\n 'thesecodesreduceerrors',\n 'buttheyalsoreducetherateatwhichinformationiscommunicated',\n 'moregenerally',\n 'anymethodwhichreducestheeﬀectsofnoisealsoreducestherateatwhichinformationcanbecommunicated',\n 'takingthislineofreasoningtoitslogicalconclusionseemstoimplythattheonlywaytocommunicateinformationwithzeroerroristoreducetheeﬀectiverateofinformationtransmissiontozero',\n 'andinshannon’sdaythiswaswidelybelievedtobetrue',\n 'butshannonprovedthatinformationcanbecommunicated',\n 'withvanishinglysmallerror',\n 'ataratewhichislimitedonlybythechannelcapacity',\n 'nowwegiveshannon’sfundamentaltheoremforadiscretechannelwithnoise',\n 'alsoknownasthesecondfundamentalcodingtheorem',\n 'andasshannon’snoisychannelcodingtheorem[2]:13',\n 'letadiscretechannelhavethecapacitycandadiscretesourcetheentropypersecondh.ifh≤cthereexistsacodingsystemsuchthattheoutputofthesourcecanbetransmittedoverthechannelwithanarbitrarilysmallfrequencyoferrors(oranarbitrarilysmallequivocation).ifh≥citispossibletoencodethesourcesothattheequivocationislessthanh−c+\\x01where\\x01isarbitrarilysmall',\n 'thereisnomethodofencodingwhichgivesanequivocationlessthanh−c.(theword‘equivocation’meanstheaverageuncertaintythatremainsregardingthevalueoftheinputaftertheoutputisobserved',\n 'i.e.theconditionalentropyh(x|y)).inessence',\n 'shannon’stheoremstatesthatitispossibletouseacommunicationchanneltocommunicateinformationwithalowerrorrate\\x01(epsilon),ataratearbitrarilyclosetothechannelcapacityofcbits',\n 'butitisnotpossibletocommunicateinformationatarategreaterthancbits',\n 'thecapacityofanoisychannelisdeﬁnedasc',\n 'maxp(x)i(x',\n 'y)(17)=maxp(x)[h(y)−h(y|x)]bits.(18)ifthereisnonoise(i.e.ifh(y|x)=0)thenthisreducestoequation10,whichisthecapacityofanoiselesschannel',\n 'thedataprocessinginequalitystatesthat',\n 'nomatterhowsophisticatedanydeviceis',\n 'theamountofinformationi(x',\n 'y)initsoutputaboutitsinputcannotbegreaterthantheamountofinformationh(x)intheinput.12thegaussianchannelifthenoisevaluesinachannelaredrawnindependentlyfromagaussiandistribution(i.e.η∼n(µη',\n 'vη),asdeﬁnedinequation6)thenthisdeﬁnesagaussianchannel',\n 'giventhaty',\n 'ifwewantp(y)tobegaussianthenweshouldensurethatp(x)andp(η)aregaussian',\n 'becausethesumoftwoindependentgaussianvariablesisalsogaussian[3].so',\n 'p(x)mustbe(iid)gaussianinordertomaximiseh(x),whichmaximisesh(y),whichmaximisesi(x',\n 'y).thus',\n 'ifeachinput',\n 'output',\n 'andnoisevariableis(iid)gaussianthentheaverageamountofinformationcommunicatedperoutputvalueisthechannelcapacity',\n 'sothati(x',\n 'y)=cbits',\n 'thisisaninformalstatementofshannon’scontinuousnoisychannelcodingtheoremforgaussianchannels',\n 'wecanusethistoexpresscapacityintermsoftheinput',\n 'output',\n 'andnoise.14',\n 'ifthechannelinputx∼n(µx',\n 'vx)thenthecontinuousanalogue(integral)ofequation4yieldsthediﬀerentialentropyh(x)=(1/2)log2πevxbits.(19)thedistinctionbetweendiﬀerentialentropyeﬀectivelydisappearswhenconsideringthediﬀerencebetweenentropies',\n 'andwewillthereforeﬁndthatwecansafelyignorethisdistinctionhere',\n 'giventhatthechannelnoiseisiidgaussian',\n 'itsentropyish(η)=(1/2)log2πevηbits.(20)becausetheoutputisthesumy',\n 'itisalsoiidgaussianwithvariancevy',\n 'vx+vη',\n 'anditsentropyish(y)=(1/2)log2πe(vx+vη)bits.(21)substitutingequations20and21intoequation16yieldsi(x',\n 'y)=12log(cid:18)1+vxvη(cid:19)bits,(22)whichallowsustochooseoneoutofm=2iequiprobablevalues',\n 'foragaussianchannel',\n 'y)attainsitsmaximalvalueofcbits',\n 'thevarianceofanysignalwithameanofzeroisequaltoitspower',\n 'whichistherateatwhichenergyisexpendedpersecond',\n 'andthephysicalunitofpowerismeasuredinjoulespersecond(j',\n 's)orwatts',\n 'where1watt=1j',\n 'accordingly',\n 'thesignalpoweriss',\n 'andthenoisepowerisn',\n 'thisyieldsshannon’sfamousequationforthe05101520signal',\n 'power',\n 's00.511.522.5channel',\n 'capacity',\n 'sfigure9',\n 'gaussianchannelcapacityc(equation23)increasesslowlywithsignalpowers',\n 'whichequalssignalpowerherebecausen=1.15',\n 'capacityofagaussianchannelc=12log(cid:18)1+sn(cid:19)bits,(23)wheretheratioofvariancess',\n 'nisthesignaltonoiseratio(snr),asinfigure9.itisworthnotingthat',\n 'givenagaussiansignalobscuredbygaussiannoise',\n 'theprobabilityofdetectingthesignalis[5]p=12log(cid:32)1+erf(cid:32)(cid:114)s8n(cid:33)(cid:33),(24)whereerfisthecumulativedistributionfunctionofagaussian.13fourieranalysisifasinusoidalsignalhasaperiodofλsecondsthenithasafrequencyoff=1',\n 'λperiodspersecond',\n 'measuredinhertz(hz).asinusoidwithafrequencyofwhzcanberepresentedperfectlyifitsvalueismeasuredatthenyquistsamplerate[6]of2wtimespersecond',\n 'indeed',\n 'fourieranalysisallowsalmostanysignalxtoberepresentedasamixtureofsinusoidalfouriercomponentsx(f):(f=0,',\n ',w),showninfigure10(seesection13).asignalwhichincludesfrequenciesbetween0hzandwhzhasabandwidthofwhz',\n 'fourieranalysis',\n 'fourieranalysisallowsanysignaltoberepresentedasaweightedsumofsineandcosinefunctions(seesection13).moreformally',\n 'considerasignalxwithavalueabcdfigure10',\n 'fourieranalysisdecomposesthesignalxindintoauniquesetofsinusoidalfouriercomponentsx(f)(f=0,',\n ',whz)ina',\n 'whered',\n 'a+b+c.16',\n 'xtattimet',\n 'whichspansatimeintervaloftseconds',\n 'thissignalcanberepresentedasaweightedaverageofsineandcosinefunctionsxt',\n 'x0+∞(cid:88)n=1ancos(fnt)+∞(cid:88)n=1bnsin(fnt),(25)wherefn=2πn',\n 'trepresentsfrequency',\n 'anisthefouriercoeﬃcient(amplitude)ofacosinewithfrequencyfn',\n 'andbnisthefouriercoeﬃcientofasinewithfrequencyfn;andx0representsthebackgroundamplitude(usuallyassumedtobezero).takenoverallfrequencies',\n 'thesepairsofcoeﬃcientsrepresentthefouriertransformofx',\n 'thefouriercoeﬃcientscanbefoundfromtheintegralsan=2t(cid:90)t0xtcos(fnt)dt(26)bn=2t(cid:90)t0xtsin(fnt)dt.(27)eachcoeﬃcientanspeciﬁeshowmuchofthesignalxconsistsofacosineatthefrequencyfn',\n 'andbnspeciﬁeshowmuchconsistsofasine',\n 'eachpairofcoeﬃcientsspeciﬁesthepowerandphaseofonefrequencycomponent;thepoweratfrequencyfnissf=(a2n+b2n),andthephaseisarctan(bn',\n 'an).ifxhasabandwidthofwhzthenitspowerspectrumisthesetofwvaluess0,',\n ',sw.anextremelyusefulpropertyoffourieranalysisisthat',\n 'whenappliedtoanyvariable',\n 'theresultantfouriercomponentsaremutuallyuncorrelated[7],and',\n 'whenappliedtoanygaussianvariable',\n 'thesefouriercomponentsarealsomutuallyindependent',\n 'thismeansthattheentropyofanygaussianvariablecanbeestimatedbyaddinguptheentropiesofitsfouriercomponents',\n 'whichcanbeusedtoestimatethemutualinformationbetweengaussianvariables',\n 'consideravariabley',\n 'whichisthesumofagaussiansignalxwithvariances',\n 'andgaussiannoisewithvariancen.ifthehighestfrequencyinyiswhz',\n 'andifvaluesofxaretransmittedatthenyquistrateof2whz',\n 'thenthechannelcapacityis2wcbitspersecond,(wherecisdeﬁnedinequation23).thus',\n 'whenexpressedintermsofbitspersecond',\n 'thisyieldsachannelcapacityofc',\n 'wlog(cid:18)1+sn(cid:19)bits',\n 's.(28)ifthesignalpoweroffouriercomponentx(f)iss(f),andthenoisepowerofcomponentη(f)isn(f)thenthesignaltonoiseratioiss(f)/n(f)(seesection13).themutualinformationatfrequencyfisthereforei(x(f),y(f))=log(cid:18)1+s(f)n(f)(cid:19)bits',\n 's.(29)17',\n 'becausethefouriercomponentsofanygaussianvariablearemutuallyindependent',\n 'themutualinformationbetweengaussianvariablescanbeobtainedbysummingi(x(f),y(f))overfrequencyi(x',\n 'y)=(cid:90)wf=0i(x(f),y(f))dfbits',\n 's.(30)ifeachgaussianvariablex',\n 'yandηisalsoiidtheni(x',\n 'y)=cbits',\n 'otherwisei(x',\n 'y)<cbits',\n 's[2].ifthepeakpoweratallfrequenciesisaconstantkthenitcanbeshownthati(x',\n 'y)ismaximisedwhens(f)+n(f)=k',\n 'whichdeﬁnesaﬂatpowerspectrum',\n 'finally',\n 'ifthesignalspectrumissculptedsothatthesignalplusnoisespectrumisﬂatthenthelogarithmicrelationinequation23yieldsimproved',\n 'albeitstilldiminishing',\n 'returns[7]c∝(s',\n 'n)1/3bits',\n 's.14averyshorthistoryofinformationtheoryeventhemostgiftedscientistcannotcommandanoriginaltheoryoutofthinair',\n 'justaseinsteincouldnothavedevisedhistheoriesofrelativityifhehadnoknowledgeofnewton’swork',\n 'soshannoncouldnothavecreatedinformationtheoryifhehadnoknowledgeoftheworkofboltzmann(1875)andgibbs(1902)onthermodynamicentropy',\n 'wiener(1927)onsignalprocessing',\n 'nyquist(1928)onsamplingtheory',\n 'orhartley(1928)oninformationtransmission[8].eventhoughshannonwasnotaloneintryingtosolveoneofthekeyscientiﬁcproblemsofhistime(i.e.howtodeﬁneandmeasureinformation),hewasaloneinbeingabletoproduceacompletemathematicaltheoryofinformation',\n 'atheorythatmightotherwisehavetakendecadestoconstruct',\n 'ineﬀect',\n 'shannonsingle',\n 'handedlyacceleratedtherateofscientiﬁcprogress',\n 'anditisentirelypossiblethat',\n 'withouthiscontribution',\n 'wewouldstillbetreatinginformationasifitweresomeill',\n 'deﬁnedvitalﬂuid.15keyequationslogarithmsusebase2unlessstatedotherwise',\n 'entropyh(x)=m(cid:88)i=1p(xi)log1p(xi)bits(31)h(x)=(cid:90)xp(x)log1p(x)dxbits(32)18',\n 'jointentropyh(x',\n 'y)=m(cid:88)i=1m(cid:88)j=1p(xi',\n 'yj)log1p(xi',\n 'yj)bits(33)h(x',\n 'y)=(cid:90)x(cid:90)yp(y',\n 'x)log1p(y',\n 'x)dydxbits(34)h(x',\n 'y)=i(x',\n 'y)+h(x|y)+h(y|x)bits(35)conditionalentropyh(y|x)=m(cid:88)i=1m(cid:88)j=1p(xi',\n 'yj)log1p(xi|yj)bits(36)h(y|x)=m(cid:88)i=1m(cid:88)j=1p(xi',\n 'yj)log1p(yj|xi)bits(37)h(x|y)=(cid:90)y(cid:90)xp(x',\n 'y)log1p(x|y)dxdybits(38)h(y|x)=(cid:90)y(cid:90)xp(x',\n 'y)log1p(y|x)dxdybits(39)h(x|y)=h(x',\n 'y)−h(y)bits(40)h(y|x)=h(x',\n 'y)−h(x)bits(41)fromwhichweobtainthechainruleforentropyh(x',\n 'y)=h(x)+h(y|x)bits(42)=h(y)+h(x|y)bits(43)marginalisationp(xi)=m(cid:88)j=1p(xi',\n 'yj),p(yj)=m(cid:88)i=1p(xi',\n 'yj)(44)p(x)=(cid:90)yp(x',\n 'p(y)=(cid:90)xp(x',\n 'y)dx(45)19',\n 'mutualinformationi(x',\n 'y)=m(cid:88)i=1m(cid:88)j=1p(xi',\n 'yj)logp(xi',\n 'yj)p(xi)p(yj)bits(46)i(x',\n 'y)=(cid:90)y(cid:90)xp(x',\n 'y)logp(x',\n 'y)p(x)p(y)dxdybits(47)i(x',\n 'y)=h(x)+h(y)−h(x',\n 'y)(48)=h(x)−h(x|y)(49)=h(y)−h(y|x)(50)=h(x',\n 'y)−[h(x|y)+h(y|x)]bits(51)ify',\n 'withxandy(notnecessarilyiid)gaussianvariablestheni(x',\n 'y)=(cid:90)wf=0log(cid:18)1+s(f)n(f)(cid:19)dfbits',\n 's,(52)wherewisthebandwidth',\n 's(f)/n(f)isthesignaltonoiseratioofthesignalandnoisefouriercomponentsatfrequencyf(section13),anddataaretransmittedatthenyquistrateof2wsamples',\n 'channelcapacityc',\n 'maxp(x)i(x',\n 'y)bitspervalue.(53)ifthechannelinputxhasvariances',\n 'thenoiseηhasvariancen',\n 'andbothxandηareiidgaussianvariablestheni(x',\n 'wherec=12log(cid:18)1+sn(cid:19)bitspervalue,(54)wheretheratioofvariancess',\n 'nisthesignaltonoiseratio',\n 'furtherreadingapplebaumd(2008)[9].probabilityandinformation',\n 'anintegratedapproach',\n 'athoroughintroductiontoinformationtheory',\n 'whichstrikesagoodbalancebetweenintuitiveandtechnicalexplanations',\n 'averyj(2012)[10].informationtheoryandevolution',\n 'anengagingaccountofhowinformationtheoryisrelevanttoawiderangeofnaturalandman',\n 'madesystems',\n 'including20',\n 'evolution',\n 'physics',\n 'cultureandgenetics',\n 'includesinterestingbackgroundstoriesonthedevelopmentofideaswithinthesediﬀerentdisciplines',\n 'baeyerhv(2005)[11].information',\n 'thenewlanguageofscienceerudite',\n 'range',\n 'andinsightfulaccountofinformationtheory',\n 'containsnoequations',\n 'whichmakesitveryreadable',\n 'covertandthomasj(1991)[12].elementsofinformationtheory',\n 'comprehensive',\n 'andhighlytechnical',\n 'withhistoricalnotesandanequationsummaryattheendofeachchapter',\n 'ghahramaniz(2002).informationtheory',\n 'encyclopediaofcognitivescience',\n 'anexcellent',\n 'briefoverviewofinformation',\n 'gleickj(2012)[13].theinformation',\n 'aninformalintroductiontothehistoryofideasandpeopleassociatedwithinformationtheory',\n 'guizzoem(2003)[14].theessentialmessage',\n 'claudeshannonandthemakingofinformationtheory',\n 'master’sthesis',\n 'massachusettsinstituteoftechnology',\n 'oneofthefewaccountsofshannon’sroleinthedevelopmentofinformationtheory',\n 'bitstream',\n 'handle/1721.1/39429/54526133.pdf',\n 'laughlin',\n 'sb(2006).thehungryeye',\n 'energy',\n 'informationandretinalfunction',\n 'excellentlectureontheenergycostofshannoninformationineyes',\n 'guest',\n 'talks',\n 'guest',\n 'lecturer',\n 'simon',\n 'laughlin',\n 'mackaydjc(2003)[15].informationtheory',\n 'inference',\n 'andlearningalgorithms',\n 'themodernclassiconinformationtheory',\n 'averyreadabletextthatroamsfarandwideovermanytopics',\n 'thebook’swebsite(below)alsohasalinktoanexcellentseriesofvideolecturesbymackay',\n 'mackay',\n 'itila/.piercejr(1980)[8].anintroductiontoinformationtheory',\n 'symbol',\n 'signalsandnoise',\n 'secondedition',\n 'piercewriteswithaninformal',\n 'tutorialstyleofwriting',\n 'butdoesnotﬂinchfrompresentingthefundamentaltheoremsofinformationtheory',\n 'thisbookprovidesagoodbalancebetweenwordsandequations',\n 'rezafm(1961)[3].anintroductiontoinformationtheory',\n 'amorecomprehensiveandmathematicallyrigorousbookthanpierce’sbook',\n 'itshouldbereadonlyafterﬁrstreadingpierce’smoreinformaltext',\n 'seifec(2007)[16].decodingtheuniverse',\n 'howthenewscienceofinformationisexplainingeverythinginthecosmos',\n 'fromourbrainstoblackholes',\n 'alucidand21',\n 'engagingaccountoftherelationshipbetweeninformation',\n 'thermodynamicentropyandquantumcomputing',\n 'highlyrecommended',\n 'shannonceandweaverw(1949)[2].themathematicaltheoryofcommunication',\n 'universityofillinoispress',\n 'asurprisinglyaccessiblebook',\n 'writteninanerawheninformationtheorywasknownonlytoaprivilegedfew',\n 'thisbookcanbedownloadedfromhttp://cm.bell',\n 'shannonday',\n 'paper.htmlsoni',\n 'jandgoodman',\n 'r(2017)[17].amindatplay',\n 'howclaudeshannoninventedtheinformationageabiographyofshannon',\n 'stone',\n 'jv(2015)[4]informationtheory',\n 'atutorialintroduction',\n 'amoreextensiveintroductionthanthecurrentarticle',\n 'forthecompletenovice',\n 'thevideosattheonlinekahnacademyprovideanexcellentintroduction',\n 'additionally',\n 'theonlinescholarpediawebpagebylathamandrudiprovidesalucidtechnicalaccountofmutualinformation',\n 'article',\n 'mutual_information',\n 'finally',\n 'somehistoricalperspectiveisprovidedinalonginterviewwithshannonconductedin1982',\n 'index.php',\n 'history',\n 'claude_e._shannon',\n 'references[1]ceshannon',\n 'amathematicaltheoryofcommunication',\n 'bellsystemtechnicaljournal,27:379–423,1948.[2]ceshannonandwweaver',\n 'themathematicaltheoryofcommunication',\n 'universityofillinoispress,1949.[3]fmreza',\n 'informationtheory',\n 'newyork',\n 'mcgraw',\n 'hill,1961.[4]jvstone',\n 'informationtheory',\n 'atutorialintroduction',\n 'sebtelpress',\n 'sheﬃeld',\n 'england,2015.[5]srschultz',\n 'signal',\n 'noiseratioinneuroscience',\n 'scholarpedia,2(6):2046,2007.[6]h.nyquist',\n 'certaintopicsintelegraphtransmissiontheory',\n 'proceedingsoftheieee,90(2):280–305,1928.[7]frieke',\n 'dwarland',\n 'rrderuytervansteveninck',\n 'andwbialek',\n 'spike',\n 'exploringtheneuralcode',\n 'mitpress',\n 'cambridge',\n 'ma,1997.22',\n '8]jrpierce',\n 'anintroductiontoinformationtheory',\n 'symbol',\n 'signalsandnoise',\n 'dover,1980.[9]dapplebaum',\n 'probabilityandinformationanintegratedapproach,2ndedition',\n 'cambridgeuniversitypress',\n 'cambridge,2008.[10]javery',\n 'informationtheoryandevolution',\n 'worldscientiﬁcpublishing,2012.[11]hvbaeyer',\n 'information',\n 'thenewlanguageofscience',\n 'harvarduniversitypress,2005.[12]tmcoverandjathomas',\n 'elementsofinformationtheory',\n 'newyork',\n 'johnwileyandsons,1991.[13]jgleick',\n 'theinformation',\n 'vintage,2012.[14]emguizzo',\n 'theessentialmessage',\n 'bitstream',\n 'handle/1721.1/39429/54526133.pdf',\n 'sachusettsinstituteoftechnology,2003.[15]djcmackay',\n 'informationtheory',\n 'inference',\n 'andlearningalgorithms',\n 'cambridgeuniversitypress,2003.[16]cseife',\n 'decodingtheuniverse',\n 'howthenewscienceofinformationisexplainingeverythinginthecosmos',\n 'fromourbrainstoblackholes',\n 'penguin,2007.[17]jsoniandrgoodman',\n 'amindatplay',\n 'howclaudeshannoninventedtheinformationage',\n 'simonandschuster,2017.23']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = topic_model.prepare_text_for_lda(text.decode(\"utf-8\"))\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens2 = topic_model.tokenize(text.decode(\"utf-8\"))\n",
    "tokens2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
