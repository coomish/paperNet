{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Notebook to experiment with topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/ellen/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to /Users/ellen/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.load('en')\n",
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/ellen/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to /Users/ellen/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import load_pdfs, read_pdf, topic_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Load PDFs from folder 'papers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileNames():\n",
    "    foldername = 'papers'\n",
    "    file_names = os.listdir(foldername)\n",
    "    return foldername, file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Recommender Systems.pdf\n\n\nInformation Theory - Tutorial.pdf\n\n\nGeometric Understanding of Deep Learning.pdf\n\n\n1802.05968v2.pdf\n\n\nTime Series Feature Extraction.pdf\n\n\nAn Introduction to DRL.pdf\n\n\nMultitask Learning as Multiobjective Optimization.pdf\n\n\nGANs.pdf\n\n\nIntroduction to Transfer Learning.pdf\n\n\nDeep CNN Design Patterns.pdf\n\n\n"
     ]
    }
   ],
   "source": [
    "foldername, filenames = load_pdfs.getFileNames()\n",
    "\n",
    "try:\n",
    "    for each in filenames:\n",
    "        print(each)\n",
    "        print('\\n')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Retrieve the text from the PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import XMLConverter, TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "import io\n",
    "\n",
    "def pdfparser(data):\n",
    "    fp = open(data, 'rb')\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    # Create a PDF interpreter object.\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    # Process each page contained in the document.\n",
    "\n",
    "    for page in PDFPage.get_pages(fp):\n",
    "        interpreter.process_page(page)\n",
    "        data = retstr.getvalue()\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pdfparser(foldername+'/'+filenames[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic model part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/ellen/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to /Users/ellen/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.load('en')\n",
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "\n",
    "\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)\n",
    "\n",
    "\n",
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = topic_model.prepare_text_for_lda(text)\n",
    "# tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens2 = topic_model.tokenize(text)\n",
    "# tokens2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "information;133\nentropy;84\nchannel;74\nvariable;51\nshannon;47\nnoise;47\ninput;47\noutput;43\ntheory;42\nvalue;39\ncapacity;32\nvalues;31\ngaussian;31\nsignal;30\ndistribution;29\nbinary;24\naverage;24\nfigure;23\ndigit;23\npossible;23\noutcome;23\namount;21\nequation;21\nrepresent;20\ngiven;19\nfrequency;19\ncommunicate;18\nequiprobable;18\nfourier;18\ntheorem;17\nprobability;16\ncid:90;16\nmaximum;16\nsecond;16\nh(y|x;16\nnumber;15\npower;15\nprovide;14\ncomponent;13\nstate;13\nyield;13\nuncertainty;13\nintroduction;12\nsurprise;12\nm(cid:88;12\ncoding;12\nreduce;12\nvariance;12\ntransmit;11\ndeﬁned;11\n"
     ]
    }
   ],
   "source": [
    "# Calculate frequency distribution\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "\n",
    "# Output top 50 words\n",
    "\n",
    "for word, frequency in fdist.most_common(50):\n",
    "    print(u'{};{}'.format(word, frequency))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spacy.load('en_core_web_sm')  \n",
    "spacy_tokens = sp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\n SPACE\ninformation NOUN\n. PUNCT\nBefore ADP\nShannon PROPN\n’s PROPN\npaper NOUN\n, PUNCT\ninformation NOUN\nhad VERB\n"
     ]
    }
   ],
   "source": [
    "for word in spacy_tokens[190:200]:  \n",
    "    print(word.text,  word.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "information\nShannon’s paper\ninformation\n"
     ]
    }
   ],
   "source": [
    "for noun in spacy_tokens[190:200].noun_chunks:  \n",
    "    print(noun.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming - Snowball stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "been --> been\nviewed --> view\nas --> as\na --> a\nkind --> kind\nof --> of\npoorly --> poor\n\n\n --> \n\n\ndeﬁned --> deﬁn\nmiasmic --> miasmic\nﬂuid --> ﬂuid\n. --> .\nBut --> but\nafter --> after\nShannon --> shannon\n’s --> ’s\npaper --> paper\n, --> ,\nit --> it\nbecame --> becam\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "test_tokens = ['compute', 'computer', 'computed', 'computing']\n",
    "# document = sp(spacy_tokens[200:300])\n",
    "id_sequence = map(lambda x: x.orth, [token for token in spacy_tokens[200:220]])\n",
    "text = map(lambda x: sp.vocab[x].text, [id for id in id_sequence])\n",
    "\n",
    "for token in text:  \n",
    "    print(token + ' --> ' + stemmer.stem(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization - Spacy lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "been-->be\nviewed-->view\nas-->as\na-->a\nkind-->kind\nof-->of\npoorly-->poorly\n\n\n-->\n\n\ndeﬁned-->deﬁne\nmiasmic-->miasmic\nﬂuid-->ﬂuid\n.-->.\nBut-->but\nafter-->after\nShannon-->Shannon\n’s-->’s\npaper-->paper\n,-->,\nit-->-PRON-\nbecame-->become\n"
     ]
    }
   ],
   "source": [
    "for word in spacy_tokens[200:220]:  \n",
    "    print(str(word.text) + \"-->\" +   str(word.lemma_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def word_grams(words, number):\n",
    "    s = []\n",
    "    for ngram in ngrams(words, number):\n",
    "            s.append(' '.join(str(i) for i in ngram))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['information theory',\n 'theory tutorial',\n 'tutorial introduction',\n 'introduction james',\n 'james stone',\n 'stone psychology',\n 'psychology department',\n 'department university',\n 'university sheﬃeld',\n 'sheﬃeld england',\n 'england j.v.stone@sheﬃeld.ac.uk',\n 'j.v.stone@sheﬃeld.ac.uk informationtheory',\n 'informationtheory jvstone',\n 'jvstone v3.tex',\n 'v3.tex abstract',\n 'abstract shannon',\n 'shannon mathematical',\n 'mathematical theory',\n 'theory communication',\n 'communication deﬁnes',\n 'deﬁnes fundamental',\n 'fundamental limit',\n 'limit information',\n 'information transmit',\n 'transmit diﬀerent',\n 'diﬀerent component',\n 'component biological',\n 'biological system',\n 'system paper',\n 'paper informal',\n 'informal rigorous',\n 'rigorous introduction',\n 'introduction idea',\n 'idea implicit',\n 'implicit shannon',\n 'shannon theory',\n 'theory annotate',\n 'annotate reading',\n 'reading provide',\n 'provide reading',\n 'reading introduction',\n 'introduction claude',\n 'claude shannon',\n 'shannon publish',\n 'publish paper',\n 'paper call',\n 'call mathematical',\n 'mathematical theory',\n 'theory communication[1].',\n 'communication[1]. paper',\n 'paper herald',\n 'herald transformation',\n 'transformation understanding',\n 'understanding information',\n 'information shannon',\n 'shannon paper',\n 'paper information',\n 'information view',\n 'view poorly',\n 'poorly deﬁned',\n 'deﬁned miasmic',\n 'miasmic shannon',\n 'shannon paper',\n 'paper become',\n 'become apparent',\n 'apparent information',\n 'information deﬁned',\n 'deﬁned measurable',\n 'measurable quantity',\n 'quantity indeed',\n 'indeed note',\n 'note shannon',\n 'shannon basic',\n 'basic information',\n 'information theory',\n 'theory information',\n 'information treat',\n 'treat physical',\n 'physical quantity',\n 'quantity energy',\n 'energy caude',\n 'caude shannon',\n 'shannon message',\n 'message noise',\n 'noise message',\n 'message encoding',\n 'encoding channel',\n 'channel decoding',\n 'decoding figure',\n 'figure communication',\n 'communication channel',\n 'channel message',\n 'message encode',\n 'encode input',\n 'input communication',\n 'communication channel',\n 'channel noise',\n 'noise channel',\n 'channel output',\n 'output decode',\n 'decode receiver',\n 'receiver recover',\n 'recover message',\n 'message information',\n 'information theory',\n 'theory deﬁnes',\n 'deﬁnes deﬁnite',\n 'deﬁnite unbreachable',\n 'unbreachable limit',\n 'limit precisely',\n 'precisely information',\n 'information communicate',\n 'communicate component',\n 'component system',\n 'system whether',\n 'whether system',\n 'system natural',\n 'natural theorem',\n 'theorem information',\n 'information theory',\n 'theory important',\n 'important deserve',\n 'deserve regard',\n 'regard information[2',\n 'information[2 basic',\n 'basic information',\n 'information summarise',\n 'summarise follow',\n 'follow communication',\n 'communication channel',\n 'channel figure',\n 'figure deﬁnite',\n 'deﬁnite upper',\n 'upper limit',\n 'limit channel',\n 'channel capacity',\n 'capacity amount',\n 'amount information',\n 'information communicate',\n 'communicate channel',\n 'channel limit',\n 'limit shrink',\n 'shrink amount',\n 'amount noise',\n 'noise channel',\n 'channel increase',\n 'increase limit',\n 'limit nearly',\n 'nearly reach',\n 'reach judicious',\n 'judicious packaging',\n 'packaging encoding',\n 'encoding finding',\n 'finding route',\n 'route information',\n 'information usually',\n 'usually measure',\n 'measure information',\n 'information allow',\n 'allow choose',\n 'choose equally',\n 'equally probable',\n 'probable equiprobable',\n 'equiprobable alternative',\n 'alternative order',\n 'order understand',\n 'understand imagine',\n 'imagine standing',\n 'standing point',\n 'point figure',\n 'figure point',\n 'point mark',\n 'mark represent',\n 'represent equiprobable',\n 'equiprobable alternative',\n 'alternative receive',\n 'receive information',\n 'information represent',\n 'represent instruction',\n 'instruction binary',\n 'binary digit',\n 'digit right',\n 'right binary',\n 'binary digit',\n 'digit provide',\n 'provide information',\n 'information tell',\n 'tell choose',\n 'choose imagine',\n 'imagine another',\n 'another point',\n 'point figure',\n 'figure binary',\n 'binary digit',\n 'digit right',\n 'right provide',\n 'provide information',\n 'information allow',\n 'allow choose',\n 'choose correct',\n 'correct lead',\n 'lead possible',\n 'possible interim',\n 'interim destination',\n 'destination could',\n 'could figure',\n 'figure traveller',\n 'traveller require',\n 'require information',\n 'information correct',\n 'correct decision',\n 'decision right',\n 'right summarise',\n 'summarise instructions',\n 'instructions need',\n 'need arrive',\n 'arrive destination',\n 'destination indicate',\n 'indicate right',\n 'right 7c---------------',\n '7c--------------- ----------------------leftright01001111110000',\n '----------------------leftright01001111110000 reach',\n 'reach making',\n 'making decision',\n 'decision binary',\n 'binary digit',\n 'digit allow',\n 'allow correct',\n 'correct decision',\n 'decision provide',\n 'provide information',\n 'information allow',\n 'allow choose',\n 'choose equiprobable',\n 'equiprobable alternative',\n 'alternative equal',\n 'equal third',\n 'third binary',\n 'binary digit',\n 'digit right',\n 'right provide',\n 'provide information',\n 'information allow',\n 'allow choose',\n 'choose correct',\n 'correct leading',\n 'leading point',\n 'point mark',\n 'mark eight',\n 'eight roads',\n 'roads could',\n 'could chosen',\n 'chosen start',\n 'start three',\n 'three binary',\n 'binary digit',\n 'digit provide',\n 'provide three',\n 'three information',\n 'information allow',\n 'allow choose',\n 'choose eight',\n 'eight equiprobable',\n 'equiprobable alternative',\n 'alternative equal',\n 'equal restate',\n 'restate general',\n 'general terms',\n 'terms represent',\n 'represent number',\n 'number fork',\n 'fork represent',\n 'represent number',\n 'number destination',\n 'destination fork',\n 'fork eﬀectively',\n 'eﬀectively chosen',\n 'chosen destination',\n 'destination decision',\n 'decision require',\n 'require information',\n 'information fork',\n 'fork require',\n 'require information',\n 'information view',\n 'view another',\n 'another perspective',\n 'perspective possible',\n 'possible destination',\n 'destination number',\n 'number fork',\n 'fork logarithm',\n 'logarithm number',\n 'number fork',\n 'fork imply',\n 'imply eight',\n 'eight destination',\n 'destination generally',\n 'generally logarithm',\n 'logarithm power',\n 'power raise',\n 'raise order',\n 'order obtain',\n 'obtain equivalently',\n 'equivalently given',\n 'given number',\n 'number express',\n 'express logarithm',\n 'logarithm subscript',\n 'subscript indicate',\n 'indicate using',\n 'using logarithm',\n 'logarithm unless',\n 'unless state',\n 'state otherwise',\n 'otherwise binary',\n 'binary digit',\n 'digit derive',\n 'derive binary',\n 'binary digit',\n 'digit binary',\n 'binary digit',\n 'digit fundamentally',\n 'fundamentally diﬀerent',\n 'diﬀerent type',\n 'type quantity',\n 'quantity binary',\n 'binary digit',\n 'digit value',\n 'value binary',\n 'binary variable',\n 'variable whereas',\n 'whereas amount',\n 'amount information',\n 'information mistake',\n 'mistake binary',\n 'binary digit',\n 'digit category',\n 'category error',\n 'error category',\n 'category error',\n 'error mistaking',\n 'mistaking marzipan',\n 'marzipan justice',\n 'justice analogous',\n 'analogous mistaking',\n 'mistaking size',\n 'size bottle',\n 'bottle bottle',\n 'bottle contain',\n 'contain binary',\n 'binary digit',\n 'digit average',\n 'average possible',\n 'possible state',\n 'state convey',\n 'convey information',\n 'information information',\n 'information entropy',\n 'entropy consider',\n 'consider land',\n 'land head',\n 'head ﬂipped',\n 'ﬂipped expect',\n 'expect head',\n 'head surprise',\n 'surprise land',\n 'land tails',\n 'tails improbable',\n 'improbable particular',\n 'particular outcome',\n 'outcome surprise',\n 'surprise observe',\n 'observe logarithm',\n 'logarithm shannon',\n 'shannon information',\n 'information surprisal',\n 'surprisal outcome',\n 'outcome measure',\n 'measure figure',\n 'figure shannon',\n 'shannon information',\n 'information often',\n 'often express',\n 'express information',\n 'information entropy',\n 'entropy average',\n 'average shannon',\n 'shannon information',\n 'information represent',\n 'represent outcome',\n 'outcome random',\n 'random variable',\n 'variable practice',\n 'practice usually',\n 'usually interest',\n 'interest surprise',\n 'surprise particular',\n 'particular value',\n 'value random',\n 'random variable',\n 'variable interest',\n 'interest surprise',\n 'surprise average',\n 'average associate',\n 'associate entire',\n 'entire possible',\n 'possible values',\n 'values average',\n 'average surprise',\n 'surprise variable',\n 'variable deﬁned',\n 'deﬁned probability',\n 'probability distribution',\n 'distribution call',\n 'call entropy',\n 'entropy represent',\n 'represent entropy',\n 'entropy average',\n 'average amount',\n 'amount surprise',\n 'surprise possible',\n 'possible outcome',\n 'outcome found',\n 'found follow',\n 'follow unbiased',\n 'unbiased shannon',\n 'shannon information',\n 'information gain',\n 'gain observe',\n 'observe 1/0.5',\n '1/0.5 average',\n 'average shannon',\n 'shannon information',\n 'information gain',\n 'gain entropy',\n 'entropy deﬁned',\n 'deﬁned average',\n 'average shannon',\n 'shannon information',\n 'information entropy',\n 'entropy entropy',\n 'entropy unfair',\n 'unfair bias',\n 'bias bias',\n 'bias probability',\n 'probability predict',\n 'predict result',\n 'result accuracy',\n 'accuracy predict',\n 'predict outcome',\n 'outcome amount',\n 'amount shannon',\n 'shannon information',\n 'information gain',\n 'gain log(1/0.9',\n 'log(1/0.9 outcome',\n 'outcome figure',\n 'figure shannon',\n 'shannon information',\n 'information surprise',\n 'surprise values',\n 'values probable',\n 'probable larger',\n 'larger values',\n 'values surprise',\n 'surprise deﬁned',\n 'deﬁned log2(1',\n 'log2(1 graph',\n 'graph entropy',\n 'entropy versus',\n 'versus probability',\n 'probability entropy',\n 'entropy average',\n 'average amount',\n 'amount surprise',\n 'surprise shannon',\n 'shannon information',\n 'information distribution',\n 'distribution possible',\n 'possible outcome',\n 'outcome head',\n 'head tails',\n 'tails 00.20.40.60.81',\n '00.20.40.60.81 p(x)01234567surprise',\n 'p(x)01234567surprise amount',\n 'amount shannon',\n 'shannon information',\n 'information gain',\n 'gain log(1/0.1',\n 'log(1/0.1 notice',\n 'notice information',\n 'information associate',\n 'associate surprise',\n 'surprise outcome',\n 'outcome given',\n 'given proportion',\n 'proportion yield',\n 'yield proportion',\n 'proportion yield',\n 'yield average',\n 'average surprise',\n 'surprise come',\n 'come 0.469',\n '0.469 figure',\n 'figure deﬁne',\n 'deﬁne equation',\n 'equation write',\n 'write 2(cid:88',\n '2(cid:88 generally',\n 'generally entropy',\n 'entropy random',\n 'random variable',\n 'variable probability',\n 'probability distribution',\n 'distribution m(cid:88',\n 'm(cid:88 reason',\n 'reason deﬁnition',\n 'deﬁnition matter',\n 'matter shannon',\n 'shannon source',\n 'source coding',\n 'coding theorem',\n 'theorem section',\n 'section guarantee',\n 'guarantee value',\n 'value variable',\n 'variable represent',\n 'represent average',\n 'average binary',\n 'binary digit',\n 'digit however',\n 'however values',\n 'values consecutive',\n 'consecutive values',\n 'values random',\n 'random variable',\n 'variable independent',\n 'independent value',\n 'value predictable',\n 'predictable therefore',\n 'therefore surprise',\n 'surprise reduce',\n 'reduce information',\n 'information carry',\n 'carry capability',\n 'capability entropy',\n 'entropy variable',\n 'variable important',\n 'important specify',\n 'specify whether',\n 'whether consecutive',\n 'consecutive variable',\n 'variable values',\n 'values independent',\n 'independent interpreting',\n 'interpreting entropy',\n 'entropy variable',\n 'variable could',\n 'could represent',\n 'represent 2h(x)or',\n '2h(x)or equiprobable',\n 'equiprobable values',\n 'values similarly',\n 'similarly 0.469',\n '0.469 variable',\n 'variable figure',\n 'figure histogram',\n 'histogram outcome',\n 'outcome values',\n 'values 2345678910111200.020.040.060.080.10.120.140.160.18outcome',\n '2345678910111200.020.040.060.080.10.120.140.160.18outcome valueoutcome',\n 'valueoutcome probability',\n 'probability could',\n 'could represent',\n 'represent 20.469',\n '20.469 equiprobable',\n 'equiprobable values',\n 'values side',\n 'side sight',\n 'sight seem',\n 'seem statement',\n 'statement nevertheless',\n 'nevertheless translate',\n 'translate entropy',\n 'entropy equivalent',\n 'equivalent number',\n 'number equiprobable',\n 'equiprobable values',\n 'values serve',\n 'serve intuitive',\n 'intuitive guide',\n 'guide amount',\n 'amount information',\n 'information represent',\n 'represent variable',\n 'variable dice',\n 'dice entropy',\n 'entropy throw',\n 'throw 6-sided',\n '6-sided yield',\n 'yield outcome',\n 'outcome order',\n 'order numbers',\n 'numbers total',\n 'total equiprobable',\n 'equiprobable outcome',\n 'outcome show',\n 'show table',\n 'table deﬁne',\n 'deﬁne outcome',\n 'outcome value',\n 'value numbers',\n 'numbers possible',\n 'possible outcome',\n 'outcome values',\n 'values represent',\n 'represent symbol',\n 'symbol outcome',\n 'outcome values',\n 'values occur',\n 'occur frequency',\n 'frequency show',\n 'show figure',\n 'figure table',\n 'table divide',\n 'divide frequency',\n 'frequency outcome',\n 'outcome value',\n 'value yield',\n 'yield probability',\n 'probability outcome',\n 'outcome value',\n 'value using',\n 'using equation',\n 'equation probability',\n 'probability entropy',\n 'entropy p(x11',\n 'p(x11 p(x11',\n 'p(x11 using',\n 'using interpretation',\n 'interpretation describe',\n 'describe variable',\n 'variable entropy',\n 'entropy represent',\n 'represent 23.27',\n '23.27 equiprobable',\n 'equiprobable values',\n 'values entropy',\n 'entropy uncertainty',\n 'uncertainty entropy',\n 'entropy measure',\n 'measure uncertainty',\n 'uncertainty uncertainty',\n 'uncertainty reduce',\n 'reduce information',\n 'information information',\n 'information entropy',\n 'entropy side',\n 'side however',\n 'however information',\n 'information rather',\n 'rather subtle',\n 'subtle interpretation',\n 'interpretation easily',\n 'easily confusion',\n 'confusion average',\n 'average information',\n 'information share',\n 'share deﬁnition',\n 'deﬁnition entropy',\n 'entropy whether',\n 'whether given',\n 'given quantity',\n 'quantity information',\n 'information entropy',\n 'entropy usually',\n 'usually depend',\n 'depend whether',\n 'whether given',\n 'given take',\n 'take symbol',\n 'symbol outcome',\n 'outcome frequency',\n 'frequency surprisal',\n 'surprisal table',\n 'table possible',\n 'possible outcome',\n 'outcome outcome',\n 'outcome value',\n 'value total',\n 'total number',\n 'number given',\n 'given throw',\n 'throw outcome',\n 'outcome order',\n 'order numbers',\n 'numbers could',\n 'could generate',\n 'generate symbol',\n 'symbol number',\n 'number diﬀerent',\n 'diﬀerent outcome',\n 'outcome could',\n 'could generate',\n 'generate outcome',\n 'outcome value',\n 'value probability',\n 'probability yield',\n 'yield given',\n 'given outcome',\n 'outcome value',\n 'value freq/36',\n 'freq/36 surprisal',\n 'surprisal log(1',\n 'log(1 example',\n 'example variable',\n 'variable entropy',\n 'entropy initial',\n 'initial uncertainty',\n 'uncertainty value',\n 'value variable',\n 'variable large',\n 'large deﬁnition',\n 'deﬁnition exactly',\n 'exactly equal',\n 'equal entropy',\n 'entropy value',\n 'value variable',\n 'variable average',\n 'average given',\n 'given amount',\n 'amount information',\n 'information equal',\n 'equal uncertainty',\n 'uncertainty entropy',\n 'entropy initially',\n 'initially value',\n 'value receive',\n 'receive amount',\n 'amount information',\n 'information equivalent',\n 'equivalent exactly',\n 'exactly amount',\n 'amount entropy',\n 'entropy uncertainty',\n 'uncertainty take',\n 'take entropy',\n 'entropy continuous',\n 'continuous variable',\n 'variable discrete',\n 'discrete variable',\n 'variable entropy',\n 'entropy deﬁned',\n 'deﬁned however',\n 'however continuous',\n 'continuous variable',\n 'variable entropy',\n 'entropy eﬀectively',\n 'eﬀectively inﬁnite',\n 'inﬁnite consider',\n 'consider diﬀerence',\n 'diﬀerence discrete',\n 'discrete variable',\n 'variable possible',\n 'possible values',\n 'values continuous',\n 'continuous variable',\n 'variable uncountably',\n 'uncountably inﬁnite',\n 'inﬁnite number',\n 'number possible',\n 'possible values',\n 'values simplicity',\n 'simplicity assume',\n 'assume values',\n 'values equally',\n 'equally probable',\n 'probable probability',\n 'probability observe',\n 'observe value',\n 'value discrete',\n 'discrete variable',\n 'variable entropy',\n 'entropy contrast',\n 'contrast probability',\n 'probability observe',\n 'observe value',\n 'value continuous',\n 'continuous variable',\n 'variable entropy',\n 'entropy respect',\n 'respect make',\n 'make sense',\n 'sense value',\n 'value continuous',\n 'continuous variable',\n 'variable implicitly',\n 'implicitly speciﬁed',\n 'speciﬁed inﬁnite',\n 'inﬁnite precision',\n 'precision follow',\n 'follow amount',\n 'amount information',\n 'information convey',\n 'convey value',\n 'value inﬁnite',\n 'inﬁnite however',\n 'however result',\n 'result imply',\n 'imply continuous',\n 'continuous variable',\n 'variable entropy',\n 'entropy order',\n 'order assign',\n 'assign diﬀerent',\n 'diﬀerent values',\n 'values diﬀerent',\n 'diﬀerent variable',\n 'variable inﬁnite',\n 'inﬁnite terms',\n 'terms simply',\n 'simply ignore',\n 'ignore yield',\n 'yield diﬀerential',\n 'diﬀerential entropy',\n 'entropy cid:90',\n 'cid:90 worth',\n 'worth note',\n 'note technical',\n 'technical diﬃculties',\n 'diﬃculties associate',\n 'associate entropy',\n 'entropy continuous',\n 'continuous variable',\n 'variable disappear',\n 'disappear quantity',\n 'quantity mutual',\n 'mutual information',\n 'information involve',\n 'involve diﬀerence',\n 'diﬀerence entropy',\n 'entropy convenience',\n 'convenience entropy',\n 'entropy continuous',\n 'continuous discrete',\n 'discrete variable',\n 'variable maximum',\n 'maximum entropy',\n 'entropy distribution',\n 'distribution distribution',\n 'distribution values',\n 'values entropy',\n 'entropy information',\n 'information theoretically',\n 'theoretically possible',\n 'possible maximum',\n 'maximum entropy',\n 'entropy distribution',\n 'distribution maximum',\n 'maximum entropy',\n 'entropy distribution',\n 'distribution important',\n 'important variable',\n 'variable transmit',\n 'transmit information',\n 'information possible',\n 'possible better',\n 'better maximum',\n 'maximum entropy',\n 'entropy given',\n 'given variable',\n 'variable precise',\n 'precise maximum',\n 'maximum entropy',\n 'entropy distribution',\n 'distribution depend',\n 'depend constraint',\n 'constraint place',\n 'place values',\n 'values variable[3].',\n 'variable[3]. prove',\n 'prove useful',\n 'useful summarise',\n 'summarise three',\n 'three important',\n 'important maximum',\n 'maximum entropy',\n 'entropy distribution',\n 'distribution list',\n 'list order',\n 'order decrease',\n 'decrease numbers',\n 'numbers constraint',\n 'constraint gaussian',\n 'gaussian distribution',\n 'distribution variable',\n 'variable variance',\n 'variance otherwise',\n 'otherwise unconstrained',\n 'unconstrained maximum',\n 'maximum entropy',\n 'entropy distribution',\n 'distribution gaussian',\n 'gaussian distribution',\n 'distribution figure',\n 'figure particularly',\n 'particularly important',\n 'important terms',\n 'terms energy',\n 'energy eﬃciency',\n 'eﬃciency distribution',\n 'distribution provide',\n 'provide information',\n 'information lower',\n 'lower energy',\n 'energy variable',\n 'variable gaussian',\n 'gaussian normal',\n 'normal distribution',\n 'distribution probability',\n 'probability observe',\n 'observe particular',\n 'particular value',\n 'value e−(µx−x)2/(2vx',\n 'e−(µx−x)2/(2vx 2.7183',\n '2.7183 equation',\n 'equation deﬁnes',\n 'deﬁnes shape',\n 'shape curve',\n 'curve figure',\n 'figure variable',\n 'variable deﬁnes',\n 'deﬁnes central',\n 'central value',\n 'value distribution',\n 'distribution assume',\n 'assume variable',\n 'variable unless',\n 'unless state',\n 'state otherwise',\n 'otherwise variance',\n 'variance variable',\n 'variable square',\n 'square standard',\n 'standard deviation',\n 'deviation deﬁnes',\n 'deﬁnes width',\n 'width curve',\n 'curve equation',\n 'equation probability',\n 'probability density',\n 'density function',\n 'function strictly',\n 'strictly speaking',\n 'speaking probability',\n 'probability density',\n 'density exponential',\n 'exponential distribution',\n 'distribution variable',\n 'variable values',\n 'values otherwise',\n 'otherwise unconstrained',\n 'unconstrained maximum',\n 'maximum entropy',\n 'entropy distribution',\n 'distribution exponential',\n 'exponential e−x/µ',\n 'e−x/µ variance',\n 'variance var(x',\n 'var(x show',\n 'show figure',\n 'figure uniform',\n 'uniform distribution',\n 'distribution variable',\n 'variable lower',\n 'lower bound',\n 'bound upper',\n 'upper bound',\n 'bound otherwise',\n 'otherwise unconstrained',\n 'unconstrained maximum',\n 'maximum entropy',\n 'entropy distribution',\n 'distribution uniform',\n 'uniform 1/(xmax',\n '1/(xmax figure',\n 'figure maximum',\n 'maximum entropy',\n 'entropy distribution',\n 'distribution gaussian',\n 'gaussian distribution',\n 'distribution standard',\n 'standard deviation',\n 'deviation equation',\n 'equation exponential',\n 'exponential distribution',\n 'distribution indicate',\n 'indicate vertical',\n 'vertical equation',\n ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_grams(tokens, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
