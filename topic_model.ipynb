{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Notebook to experiment with topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/ellen/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to /Users/ellen/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.load('en')\n",
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_pdfs, read_pdf, topic_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Load PDFs from folder 'papers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileNames():\n",
    "    foldername = 'papers'\n",
    "    file_names = os.listdir(foldername)\n",
    "    return foldername, file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Recommender Systems.pdf\n\n\nInformation Theory - Tutorial.pdf\n\n\nGeometric Understanding of Deep Learning.pdf\n\n\n1802.05968v2.pdf\n\n\nTime Series Feature Extraction.pdf\n\n\nAn Introduction to DRL.pdf\n\n\nMultitask Learning as Multiobjective Optimization.pdf\n\n\nGANs.pdf\n\n\nIntroduction to Transfer Learning.pdf\n\n\nDeep CNN Design Patterns.pdf\n\n\n"
     ]
    }
   ],
   "source": [
    "foldername, filenames = load_pdfs.getFileNames()\n",
    "\n",
    "try:\n",
    "    for each in filenames:\n",
    "        print(each)\n",
    "        print('\\n')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Retrieve the text from the PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import XMLConverter, TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "import io\n",
    "\n",
    "def pdfparser(data):\n",
    "    fp = open(data, 'rb')\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    # Create a PDF interpreter object.\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    # Process each page contained in the document.\n",
    "\n",
    "    for page in PDFPage.get_pages(fp):\n",
    "        interpreter.process_page(page)\n",
    "        data = retstr.getvalue()\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pdfparser(foldername+'/'+filenames[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = topic_model.prepare_text_for_lda(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "information;133\nentropy;84\nchannel;74\nvariable;51\nshannon;47\nnoise;47\ninput;47\noutput;43\ntheory;42\nvalue;39\n"
     ]
    }
   ],
   "source": [
    "# Calculate frequency distribution\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "\n",
    "# Output top 50 words\n",
    "\n",
    "for word, frequency in fdist.most_common(10):\n",
    "    print(u'{};{}'.format(word, frequency))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove punctuation, newline chars, stopwords etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43507\n comp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10153\n\n\nchannel (Figure 1\n"
     ]
    }
   ],
   "source": [
    "print(len(text))\n",
    "print(text[420:425])\n",
    "tokens = nlp(text)\n",
    "print(len(tokens))\n",
    "print(tokens[420:425])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "STOPLIST = set(stopwords.words('english'))\n",
    "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-\", \"...\", \"”\", \"”\"]\n",
    "punctuations = string.punctuation\n",
    "\n",
    "def clean_up(tokens):\n",
    "    clean_tokens = []\n",
    "    for each in tokens:\n",
    "        if each.is_stop == False:\n",
    "            if each.is_punct == False:\n",
    "                clean_tokens.append(each)\n",
    "    final_tokens = [tok.lemma_.lower().strip() for tok in clean_tokens if tok.lemma_ != '-PRON-']\n",
    "    final_tokens = [tok for tok in final_tokens if tok not in STOPLIST and tok not in punctuations]\n",
    "    final_tokens = ' '.join(final_tokens)\n",
    "    return final_tokens\n",
    "\n",
    "# cleaned_tokens = []\n",
    "# for each in spacy_tokens:\n",
    "#     if each.is_stop == False:\n",
    "#        cleaned_tokens.append(each)\n",
    "            \n",
    "# print(cleaned_tokens.type)\n",
    "# spacy_tokens = nlp(str(cleaned_tokens))\n",
    "# print(spacy_tokens[400:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ormation transmit diﬀerent component man biological system paper informal rigorous introduction main'"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean_text = remove_stopwords(text)\n",
    "clean_tokens = clean_up(tokens)\n",
    "clean_tokens[300:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_text = remove_symbols(strclean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove commas and \\n, and words of len 1, and \\n\\n\n",
    "\n",
    "# import string\n",
    "import re\n",
    "\n",
    "\n",
    "# print(spacy_tokens[2010:2040])\n",
    "# print(clean_text[200:300])\n",
    "# print(len(clean_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instruction NOUN\nneed VERB\narrive ADJ\ndestination NOUN\nleft VERB\nturn NOUN\nindicate VERB\n0 NUM\nright ADJ\nturn NOUN\n1 NUM\n2 NUM\nabd0 NOUN\n1 NUM\n1 NUM\n30 NUM\n0 NUM\n0 NUM\n00 NUM\n0 NUM\n1 NUM\n10 NUM\n1 NUM\n0 SYM\n21 NUM\n0 NUM\n0 NUM\n41 NUM\n0 NUM\n1 NUM\nright hand summarise instruction\narrive destination\nleft turn\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')  \n",
    "spacy_tokens = nlp(clean_tokens)\n",
    "\n",
    "# for token in doc:\n",
    "#     print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "#             token.shape_, token.is_alpha, token.is_stop)\n",
    "print(len(spacy_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming - Snowball stemmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nearly --> near\nreach --> reach\njudicious --> judici\npackaging --> packag\nencoding --> encod\ndatum --> datum\n2 --> 2\nfind --> find\nroute --> rout\nbit --> bit\nbit --> bit\ninformation --> inform\nusually --> usual\nmeasure --> measur\nbit --> bit\nbit --> bit\ninformation --> inform\nallow --> allow\nchoose --> choos\nequally --> equal\nprobable --> probabl\nequiprobable --> equiprob\nalternative --> altern\norder --> order\nunderstand --> understand\nimagine --> imagin\nstand --> stand\nfork --> fork\nroad --> road\npoint --> point\nfigure --> figur\n2 --> 2\nwant --> want\npoint --> point\nmark --> mark\nd. --> d.\nfork --> fork\nrepresent --> repres\nequiprobable --> equiprob\nalternative --> altern\ntell --> tell\nleft --> left\nreceive --> receiv\nbit --> bit\ninformation --> inform\nrepresent --> repres\ninstruction --> instruct\nbinary --> binari\ndigit --> digit\n0=left --> 0=left\n1=right --> 1=right\nbinary --> binari\ndigit --> digit\nprovide --> provid\nbit --> bit\ninformation --> inform\ntell --> tell\nroad --> road\nchoose --> choos\nimagine --> imagin\ncome --> come\nfork --> fork\npoint --> point\nb --> b\nfigure --> figur\n2 --> 2\nbinary --> binari\ndigit --> digit\n1=right --> 1=right\nprovide --> provid\nbit --> bit\ninformation --> inform\nallow --> allow\nchoose --> choos\ncorrect --> correct\nroad --> road\nlead --> lead\nc. --> c.\nnote --> note\nc --> c\npossible --> possibl\ninterim --> interim\ndestination --> destin\nfigure --> figur\n2 --> 2\ntraveller --> travel\nknow --> know\nway --> way\nfork --> fork\nroad --> road\nrequire --> requir\nbit --> bit\ninformation --> inform\ncorrect --> correct\ndecision --> decis\n0s --> 0s\n1s --> 1s\nright --> right\nhand --> hand\nsummarise --> summaris\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "# test_tokens = ['compute', 'computer', 'computed', 'computing']\n",
    "# document = sp(spacy_tokens[200:300])\n",
    "id_sequence = map(lambda x: x.orth, [token for token in spacy_tokens[200:300]])\n",
    "text = map(lambda x: sp.vocab[x].text, [id for id in id_sequence])\n",
    "\n",
    "for token in text:  \n",
    "    print(token + ' --> ' + stemmer.stem(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization - Spacy lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nearly', 'reach', 'judicious', 'packaging', 'encode', 'datum', '2', 'find', 'route', 'bit', 'bit', 'information', 'usually', 'measure', 'bit', 'bit', 'information', 'allow', 'choose', 'equally', 'probable', 'equiprobable', 'alternative', 'order', 'understand', 'imagine', 'stand', 'fork', 'road', 'point', 'figure', '2', 'want', 'point', 'mark', 'd.', 'fork', 'represent', 'equiprobable', 'alternative', 'tell', 'left', 'receive', 'bit', 'information', 'represent', 'instruction', 'binary', 'digit', '0=left', '1=right', 'binary', 'digit', 'provide', 'bit', 'information', 'tell', 'road', 'choose', 'imagine', 'come', 'fork', 'point', 'b', 'figure', '2', 'binary', 'digit', '1=right', 'provide', 'bit', 'information', 'allow', 'choose', 'correct', 'road', 'lead', 'c.', 'note', 'c', 'possible', 'interim', 'destination', 'figure', '2', 'traveller', 'know', 'way', 'fork', 'road', 'require', 'bit', 'information', 'correct', 'decision', '0s', '1s', 'right', 'hand', 'summarise']\n"
     ]
    }
   ],
   "source": [
    "for word in spacy_tokens[200:220]:  \n",
    "    pass\n",
    "    # print(str(word.text) + \"-->\" +   str(word.lemma_))\n",
    "\n",
    "spacy_lemmas = []\n",
    "for word in spacy_tokens:\n",
    "    spacy_lemmas.append(word.lemma_)\n",
    "print(spacy_lemmas[200:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instruction NOUN\nneed VERB\narrive ADJ\ndestination NOUN\nleft VERB\nturn NOUN\nindicate VERB\n0 NUM\nright ADJ\nturn NOUN\n1 NUM\n2 NUM\nabd0 NOUN\n1 NUM\n1 NUM\n30 NUM\n0 NUM\n0 NUM\n00 NUM\n0 NUM\n1 NUM\n10 NUM\n1 NUM\n0 SYM\n21 NUM\n0 NUM\n0 NUM\n41 NUM\n0 NUM\n1 NUM\nright hand summarise instruction\narrive destination\nleft turn\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for word in spacy_tokens[300:330]:  \n",
    "    print(word.text,  word.pos_)\n",
    "for noun in spacy_tokens[300:330].noun_chunks:  \n",
    "    print(noun.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", PUNCT\n' PUNCT\nv ADP\n' PUNCT\n, PUNCT\n' PUNCT\n, PUNCT\n' PUNCT\n, PUNCT\n' PUNCT\n\\n SPACE\n' PUNCT\n, PUNCT\n' PUNCT\n, PUNCT\n' PUNCT\n, PUNCT\n' PUNCT\n8 NUM\n' NOUN\n, PUNCT\n' PUNCT\n, PUNCT\n' PUNCT\n, PUNCT\n' PUNCT\n\\n SPACE\n' PUNCT\n, PUNCT\n' PUNCT\n, PUNCT\n' PUNCT\n, PUNCT\n' PUNCT\n6 NUM\n' NOUN\n, PUNCT\n' PUNCT\n, PUNCT\n' PUNCT\n, PUNCT\n' PUNCT\n\\n SPACE\n' PUNCT\n, PUNCT\n' PUNCT\n, PUNCT\n' PUNCT\n, PUNCT\n' PUNCT\n"
     ]
    }
   ],
   "source": [
    "spacy_lemmas = nlp(str(spacy_lemmas))\n",
    "for word in spacy_lemmas[250:300]:  \n",
    "    print(word.text,  word.pos_)\n",
    "for noun in spacy_lemmas[250:300].noun_chunks:  \n",
    "    print(noun.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def word_grams(words, number):\n",
    "    s = []\n",
    "    for ngram in ngrams(words, number):\n",
    "            s.append(' '.join(str(i) for i in ngram))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('information theory', 31),\n ('binary digit', 22),\n ('channel capacity', 20),\n ('log 1', 20),\n ('shannon ’s', 19),\n ('bit information', 14),\n ('variable x', 11),\n ('entropy h(x', 11),\n ('shannon information', 10),\n ('mutual information', 10)]"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = word_grams(spacy_tokens, 2)\n",
    "from collections import Counter\n",
    "count_grams = Counter(bigrams)\n",
    "count_grams.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
